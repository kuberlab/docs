{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Quick Start Guide # Kibernetika Basics # This tutorial provides a step-by-step guide to the initial setup of the Kibernetika Machine Teaching Platform , which will help you run a sample application using a template from the catalogue. This will also demonstrate the basic forms of: Infrastructure Management AI Application Management Application Deployment The steps are listed in the order they are to be performed. Kibernetika Registration # You must have an account to use Kibernetika . Once your account is created, you will be able to log into our system and connect your source code and cloud service accounts, so it can perform all cloud automation on your behalf. Go to the web page: https://cloud.kuberlab.io Sign up with your email address and other required information A confirmation email will be sent to you within 24 hours. Follow the instructions in this email to complete your registration. If you do not receive the e-mail message from the address \"noreply@kibernetika.ai\" within 24 hours of initial registration, please check the Spam folder. If you have not received the confirmation e-mail, please contact support@kibernetika.ai [Optional] Source Account Registration # If you wish to run your own model or application source code, but do not have a source account to store and host it, you can create one with any of the following services at their respective sites: Github: https://github.com/ Gitlab: https://gitlab.com/ Bitbucket: https://bitbucket.org/ If you already have a source account, you can connect it to the Kibernetika service: After logging in, go to \u2018Settings\u2019 page (click your user name in the upper right, and then \u2018Settings\u2019). You can go to the page directly using this link: https://cloud.kuberlab.io/settings/my . Under \u2018Repositories\u2019, add and configure the source account that contains your application or model source code. [Optional] Cloud Account Registration # Kibernetika provides you a shared cluster to run your applications. You can also configure to use your local Kubernetes cluster. Hence a cloud service account registration is optional. If you do not wish to use the shared cluster you will need to either configure a local cluster, or use a cloud service, and connect this to kibernetika.ai service. If you need a cloud service account to run your cluster, you can create one with the following services at their respective sites: Google: https://cloud.google.com/ AWS: https://aws.amazon.com/ To configure your cluster in the Kibernetika service: 1. Go the the 'Settings' page as before (see Source Account Registration). Under \u2018Service Accounts\u2019, add and configure the cloud service account where you will run your cluster. Currently Kibernetika supports Google Cloud, AWS, and local Kubernetes clusters. Project Creation # Before you can do anything with an application, you need to create a project. If you want to run one of the tutorials in the catalogue, you can create a tutorial project. In this case, click on the 'Catalog' button at the top. If you however want to build your own code, or use your own from your connected repository, click on your workspace from the main tab (click \u2018kibernetika.ai\u2019 at the top left) and then click the button on the right labeled 'Create new Project'. From the list of tutorials, or sample templates shown, choose the most relevant (click the 'Install' button). If you are choosing a tutorial, the 'tensorflow-tutorial-01' is a good place to start - click \"Install to My\", and then choose your workspace in the wizard that pops up. Choose your cluster. If you have created infrastructure with the optional step above, this is where you can choose your own cluster. Otherwise, you will choose the default shared cluster 'testshare' here. Click 'Next'. Name your application. Most users will leave the default name as is here. Click 'Next'. Choose the versions of various tools you need. Most users will use the default values here. Click 'Next'. Scroll down to access additional configurations The last step of template installation shows the configuration. There is no need to modify anything here. Click 'Install'. This will install the template and take you to the project screen. Running your Project # In the project screen, you will see several tabs. You can run your code from the 'Tasks' tab. This tab is used for the production flow, and allows you to define several tasks that can be run either serially or in parallel, and any number of combinations of both. The 'Sources' tab allows you to navigate the directory structure and look at all the template files. If you wish to connect your own sources, you can do this here, by adding a new source path. The 'History' tab contains a list of all the jobs you have run. 'Logs' shows the details of all the tasks started. 'Status' shows the current status of all tasks you have started. You can monitor your application from the 'Metrics' and 'Tensorboard' tabs. To add tools and frameworks packages from the open source community, use the 'Lib' tab. The 'Jupyter' allows you to step through the code and read the notes, hence is the easier method.","title":"Home"},{"location":"#quick-start-guide","text":"","title":"Quick Start Guide"},{"location":"#kibernetika-basics","text":"This tutorial provides a step-by-step guide to the initial setup of the Kibernetika Machine Teaching Platform , which will help you run a sample application using a template from the catalogue. This will also demonstrate the basic forms of: Infrastructure Management AI Application Management Application Deployment The steps are listed in the order they are to be performed.","title":"Kibernetika Basics"},{"location":"#kibernetika-registration","text":"You must have an account to use Kibernetika . Once your account is created, you will be able to log into our system and connect your source code and cloud service accounts, so it can perform all cloud automation on your behalf. Go to the web page: https://cloud.kuberlab.io Sign up with your email address and other required information A confirmation email will be sent to you within 24 hours. Follow the instructions in this email to complete your registration. If you do not receive the e-mail message from the address \"noreply@kibernetika.ai\" within 24 hours of initial registration, please check the Spam folder. If you have not received the confirmation e-mail, please contact support@kibernetika.ai","title":"Kibernetika Registration"},{"location":"#optional-source-account-registration","text":"If you wish to run your own model or application source code, but do not have a source account to store and host it, you can create one with any of the following services at their respective sites: Github: https://github.com/ Gitlab: https://gitlab.com/ Bitbucket: https://bitbucket.org/ If you already have a source account, you can connect it to the Kibernetika service: After logging in, go to \u2018Settings\u2019 page (click your user name in the upper right, and then \u2018Settings\u2019). You can go to the page directly using this link: https://cloud.kuberlab.io/settings/my . Under \u2018Repositories\u2019, add and configure the source account that contains your application or model source code.","title":"[Optional] Source Account Registration"},{"location":"#optional-cloud-account-registration","text":"Kibernetika provides you a shared cluster to run your applications. You can also configure to use your local Kubernetes cluster. Hence a cloud service account registration is optional. If you do not wish to use the shared cluster you will need to either configure a local cluster, or use a cloud service, and connect this to kibernetika.ai service. If you need a cloud service account to run your cluster, you can create one with the following services at their respective sites: Google: https://cloud.google.com/ AWS: https://aws.amazon.com/ To configure your cluster in the Kibernetika service: 1. Go the the 'Settings' page as before (see Source Account Registration). Under \u2018Service Accounts\u2019, add and configure the cloud service account where you will run your cluster. Currently Kibernetika supports Google Cloud, AWS, and local Kubernetes clusters.","title":"[Optional] Cloud Account Registration"},{"location":"#project-creation","text":"Before you can do anything with an application, you need to create a project. If you want to run one of the tutorials in the catalogue, you can create a tutorial project. In this case, click on the 'Catalog' button at the top. If you however want to build your own code, or use your own from your connected repository, click on your workspace from the main tab (click \u2018kibernetika.ai\u2019 at the top left) and then click the button on the right labeled 'Create new Project'. From the list of tutorials, or sample templates shown, choose the most relevant (click the 'Install' button). If you are choosing a tutorial, the 'tensorflow-tutorial-01' is a good place to start - click \"Install to My\", and then choose your workspace in the wizard that pops up. Choose your cluster. If you have created infrastructure with the optional step above, this is where you can choose your own cluster. Otherwise, you will choose the default shared cluster 'testshare' here. Click 'Next'. Name your application. Most users will leave the default name as is here. Click 'Next'. Choose the versions of various tools you need. Most users will use the default values here. Click 'Next'. Scroll down to access additional configurations The last step of template installation shows the configuration. There is no need to modify anything here. Click 'Install'. This will install the template and take you to the project screen.","title":"Project Creation"},{"location":"#running-your-project","text":"In the project screen, you will see several tabs. You can run your code from the 'Tasks' tab. This tab is used for the production flow, and allows you to define several tasks that can be run either serially or in parallel, and any number of combinations of both. The 'Sources' tab allows you to navigate the directory structure and look at all the template files. If you wish to connect your own sources, you can do this here, by adding a new source path. The 'History' tab contains a list of all the jobs you have run. 'Logs' shows the details of all the tasks started. 'Status' shows the current status of all tasks you have started. You can monitor your application from the 'Metrics' and 'Tensorboard' tabs. To add tools and frameworks packages from the open source community, use the 'Lib' tab. The 'Jupyter' allows you to step through the code and read the notes, hence is the easier method.","title":"Running your Project"},{"location":"about/about/","text":"Copyright@2018 Kibernetika Inc. #","title":"Copyright"},{"location":"about/about/#copyright2018-kibernetika-inc","text":"","title":"Copyright@2018 Kibernetika Inc."},{"location":"datasets/datasets/","text":"Overview # Data is the reason why analytics exists, and why we do machine learning. Without data, there is no need for any deep learning models. Since models exist only for processing and analyzing the data, the following observations about the data are true: Data needs to be validated and secure. Without this, the results of models could be invalid, or worse, even malicious. Model training is extremely time-consuming, since it happens with large amounts of data. If the data is properly pre-processed, this operation can significantly speed up the model training and increase the accuracy of the resulting model. Data changes constantly, which means that the model will need to be re-trained regularly on a different version of the data set. To generate repeatable results in model prediction, the operation needs to be done with the same data set. To use the Dataset object in the Kibernetika project, a user needs to add Dataset as a source in the project, and it will become available to the Jupyter Notebook as well as the Python or R execution environments. Create dataset # Dataset can be created from UI interface directly or using kdataset CLI . For creating new dataset from UI go to catalog in any organization, choose \"Dataset\" section in the left menu and click \"(+) Add\" button. Fill required field Name, Idenfification will be filled automatically if empty. Optional Description and Keywords also can be filled. \"Published\" checkbox should be checked if user wants his dataset will be available for all users. New dataset will be created after clicking \"Save\". Dataset as all other catalog objects can be marked with star and commented by all users who have access to this dataset. Dataset's owner can edit readme (Readme tab) and manage dataset versions (Versions tab). Update dataset metadata # Dataset metadata info can be updated only in UI. \"Edit\" option is available in dataset's context menu. User can change dataset's Name, Desctiption and Keywords. Identification can not be changed. User can change dataset's visibility option (Publish/Unpublish option): make dataset published (available for all users) or not (available only for users who can manage datasets in current organization). Also user can change dataset's picture. Fork dataset # User can use in his projects all dataset from the same with the project organization and only published datasets from other organizations despite the dataset can be available for user. So if user wants to use private dataset from another organization, he can fork this dataset to project's organization. Fork action makes dataset copy in target organization with the same name. So before forking user should ensure that there's no dataset with the same name in target organization. Also user should ensure that he can manage datasets in target organization. Fork copies only commited dataset versions . User can fork dataset by clicking \"Fork\" button and choosing target organization. Delete dataset # Dataset can be deleted both from UI (context menu \"Delete\") or using kdataset CLI . Dataset versions # All datasets are versioned. So user can create, clone and delete dataset's versions and manage version's files. Create new dataset version # Dataset version can be added in UI. User should go to \"Versions\" tab and click \"+ New Version\" button. Version name should be semantic versions. Message is optional. Versions will be added after clicking \"Create\". Also dataset version can be added with kdataset tool . Manage dataset version files # Just created dataset's version marked as Editing, it means that user can change version's contents. User can upload new files and delete existing ones. To upload file to subdirectory it needs to set required upload path to \"upload path\" field. Files are uploading with the same names as original files. For example, to upload file my-file.txt to dataset's version to directory dir/subdir user should upload file named my-file.txt with typing dir/subdir in \"upload path\" field, final uploaded file will have path dir/subdir/my-file.txt . Not existing directories will be create automatically. To delete file or directory user should click to bucket icon near object he wants to delete. When dataset version editing is finished user should commit version by clicking button \"Commit\". Version will be commited after setting the message (it's not required to change it) and confirming commit by clicking \"Commit\". Commited version can not be changed. Only commited versions can be mounted to projects. User can preview files of commited version in UI. Also commited version can be downloaded as archive. Clone version # Existing version can be cloned as new editing one by clicking button \"New version\" near existing version's name. This action looks like creating new version, but cloned version will have all files from source version. Delete version # Dataset version can be deleted from UI using context menu \"Delete\". Also dataset version can be deleted with kdataset tool . Use datasets inside project # To use the Dataset object in the Kibernetika project, a user needs to add Dataset as a source in the project, and it will become available to the Jupyter Notebook as well as the Python or R execution environments. To configure this, select the storage type \u2018dataset\u2019, and choose the required dataset and version from the list displayed (see figure below). Now you can see the data set as a folder in Jupyter Notebook. In workflow tasks it will be accessible by the use of environment variables. Both the Jupyter Notebook, and the Python and R environments can access the data files stored in the data set directories chosen. Because data sets can be very big, the loading of the data needs to be done from the command line and not the browser interface. Kibernetika has provided the kdataset utility to facilitate operations with data sets. Before loading the first version of the data set, the Dataset object needs to be created. That can be done in the GUI in the Dataset section of your workspace. Or, the data set can be created during the downloading of the data by using \u201c--create\u201d or \u201c--force\u201d flags in the \u2018push\u2019 command.","title":"Datasets Management"},{"location":"datasets/datasets/#overview","text":"Data is the reason why analytics exists, and why we do machine learning. Without data, there is no need for any deep learning models. Since models exist only for processing and analyzing the data, the following observations about the data are true: Data needs to be validated and secure. Without this, the results of models could be invalid, or worse, even malicious. Model training is extremely time-consuming, since it happens with large amounts of data. If the data is properly pre-processed, this operation can significantly speed up the model training and increase the accuracy of the resulting model. Data changes constantly, which means that the model will need to be re-trained regularly on a different version of the data set. To generate repeatable results in model prediction, the operation needs to be done with the same data set. To use the Dataset object in the Kibernetika project, a user needs to add Dataset as a source in the project, and it will become available to the Jupyter Notebook as well as the Python or R execution environments.","title":"Overview"},{"location":"datasets/datasets/#create-dataset","text":"Dataset can be created from UI interface directly or using kdataset CLI . For creating new dataset from UI go to catalog in any organization, choose \"Dataset\" section in the left menu and click \"(+) Add\" button. Fill required field Name, Idenfification will be filled automatically if empty. Optional Description and Keywords also can be filled. \"Published\" checkbox should be checked if user wants his dataset will be available for all users. New dataset will be created after clicking \"Save\". Dataset as all other catalog objects can be marked with star and commented by all users who have access to this dataset. Dataset's owner can edit readme (Readme tab) and manage dataset versions (Versions tab).","title":"Create dataset"},{"location":"datasets/datasets/#update-dataset-metadata","text":"Dataset metadata info can be updated only in UI. \"Edit\" option is available in dataset's context menu. User can change dataset's Name, Desctiption and Keywords. Identification can not be changed. User can change dataset's visibility option (Publish/Unpublish option): make dataset published (available for all users) or not (available only for users who can manage datasets in current organization). Also user can change dataset's picture.","title":"Update dataset metadata"},{"location":"datasets/datasets/#fork-dataset","text":"User can use in his projects all dataset from the same with the project organization and only published datasets from other organizations despite the dataset can be available for user. So if user wants to use private dataset from another organization, he can fork this dataset to project's organization. Fork action makes dataset copy in target organization with the same name. So before forking user should ensure that there's no dataset with the same name in target organization. Also user should ensure that he can manage datasets in target organization. Fork copies only commited dataset versions . User can fork dataset by clicking \"Fork\" button and choosing target organization.","title":"Fork dataset"},{"location":"datasets/datasets/#delete-dataset","text":"Dataset can be deleted both from UI (context menu \"Delete\") or using kdataset CLI .","title":"Delete dataset"},{"location":"datasets/datasets/#dataset-versions","text":"All datasets are versioned. So user can create, clone and delete dataset's versions and manage version's files.","title":"Dataset versions"},{"location":"datasets/datasets/#create-new-dataset-version","text":"Dataset version can be added in UI. User should go to \"Versions\" tab and click \"+ New Version\" button. Version name should be semantic versions. Message is optional. Versions will be added after clicking \"Create\". Also dataset version can be added with kdataset tool .","title":"Create new dataset version"},{"location":"datasets/datasets/#manage-dataset-version-files","text":"Just created dataset's version marked as Editing, it means that user can change version's contents. User can upload new files and delete existing ones. To upload file to subdirectory it needs to set required upload path to \"upload path\" field. Files are uploading with the same names as original files. For example, to upload file my-file.txt to dataset's version to directory dir/subdir user should upload file named my-file.txt with typing dir/subdir in \"upload path\" field, final uploaded file will have path dir/subdir/my-file.txt . Not existing directories will be create automatically. To delete file or directory user should click to bucket icon near object he wants to delete. When dataset version editing is finished user should commit version by clicking button \"Commit\". Version will be commited after setting the message (it's not required to change it) and confirming commit by clicking \"Commit\". Commited version can not be changed. Only commited versions can be mounted to projects. User can preview files of commited version in UI. Also commited version can be downloaded as archive.","title":"Manage dataset version files"},{"location":"datasets/datasets/#clone-version","text":"Existing version can be cloned as new editing one by clicking button \"New version\" near existing version's name. This action looks like creating new version, but cloned version will have all files from source version.","title":"Clone version"},{"location":"datasets/datasets/#delete-version","text":"Dataset version can be deleted from UI using context menu \"Delete\". Also dataset version can be deleted with kdataset tool .","title":"Delete version"},{"location":"datasets/datasets/#use-datasets-inside-project","text":"To use the Dataset object in the Kibernetika project, a user needs to add Dataset as a source in the project, and it will become available to the Jupyter Notebook as well as the Python or R execution environments. To configure this, select the storage type \u2018dataset\u2019, and choose the required dataset and version from the list displayed (see figure below). Now you can see the data set as a folder in Jupyter Notebook. In workflow tasks it will be accessible by the use of environment variables. Both the Jupyter Notebook, and the Python and R environments can access the data files stored in the data set directories chosen. Because data sets can be very big, the loading of the data needs to be done from the command line and not the browser interface. Kibernetika has provided the kdataset utility to facilitate operations with data sets. Before loading the first version of the data set, the Dataset object needs to be created. That can be done in the GUI in the Dataset section of your workspace. Or, the data set can be created during the downloading of the data by using \u201c--create\u201d or \u201c--force\u201d flags in the \u2018push\u2019 command.","title":"Use datasets inside project"},{"location":"how-to/cifar-10/","text":"Train CIFAR-10 Model from scratch using Kibernetika.AI # This tutorial shows basic steps required to train CIFAR-10 model using original source code from https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator Install Tensorflow project # Start creating a new project in your Workspace. More details Set Tensorflow version 1.9.0 Set source location to https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator Notes: In this case, https://github.com/tensorflow/models will be used as a Source repository and models/tutorials/image/cifar10_estimator as a subpath: Contents of this repository dir will be visible inside Jupyter or running Job under $SRC_DIR path, usually SRC_DIR is an alias for /norebooks/src Prepare Dataset # First, we need to upload CIFAR-10 to Kibernetika. There are two options to upload data. Upload data to some project directory, for example to $DATA_DIR . Upload data to Kibernetika DataSet Catalog. The second option is preferable as it will allow you to track versions of your dataset and also use the dataset in other projects. Following steps required to upload dataset to catalog: Create new task upload-dataset with resource worker for dataset uploading. Inside the project, clone an existing task or create a new one from scratch. After creating the task we are ready to define the execution command for uploading. The CIFAR-10 project already has code for that. Our task definition looks like: Basically we defined the following parameters: Execution Directory: $SRC_DIR refers to the location our CIFAR-10 source code is Execution Command: mkdir /tmp/cifar-10 && python generate_cifar10_tfrecords.py --data-dir=/tmp/cifar-10 && cd /tmp/cifar-10 && kdataset push $WORKSPACE_NAME cifar-10:1.0.0 --create Ensure that Source is mounted to your task. Open \"Advanced\" section at the bottom of form and check option \"Default volume mapping\" or add needed volume manually. During running of the new task the following steps will be executed: Make temporary directory /tmp/cifar-10 Use generate_cifar10_tfrecords.py to upload dataset to /tmp/cifar-10 Change current directory /tmp/cifar-10 Push cifar-10 dataset to current workspace DataSet catalog (use environment variable $WORKSPACE_NAME for current workspace) as version 1.0.0. Option --create means create dataset if it doesn\u2019t exist. After execution upload-dataset we can refer our data directory to the created dataset. Change definition data volume in the Sources tab to point it to the newly created dataset: Notes: kdataset command is always present in Kibernetika environment you also could push dataset to catalog directly using python script: from mlboardclient.api import client mlboard = client.Client() mlboard.datasets.push( os.environ.get('WORKSPACE_NAME'), 'cifar-10', '1.0.0', '/tmp/cifar-10', create=True, ) Standard Train model # To start training, we need to configure resource worker in task standalone to train the model. Set execution directory to $SRC_DIR Set execution command to: python cifar10_main.py --num-gpus=$GPU_COUNT --train-steps=1000 --data-dir=$DATA_DIR --job-dir=$TRAINING_DIR/$BUILD_ID Set required GPU count in the Resources section Start task Notes: $TRAINING_DIR is an alias for preconfigured training directory, see Sources tab $BUILD_ID is an alias for sequential job id, every running job has a unique id $GPU_COUNT is an alias for number of GPU allocated for execution on one compute node You can see execution logs in the Jobs tab. Use Tensorboard tab to see your training progress. The result of model training is available under training directory $TRAINING_DIR/$BUILD_ID (usually /notebooks/training/1,2\u2026 ). Distributed training # ATTENTION: CIFAR-10 Tensorflow original model is based on tf.contrib.learn which was deprecated since Tensorflow 1.7 and distributed configuration is not compatible with newer tensorflow version. We recommend migrating your code to Tensorflow Estimators. In the Kibernetika platform you can use distributed training for both old and new style models, see details below. First, we need to define resources that will be used for distributed training, e.g. workers and parameter servers. Change parallel/worker Execution Command to: TF_CONFIG=$(tf_conf worker --chief_name master) python cifar10_main.py --num-gpus=$GPU_COUNT --train-steps=1000 --data-dir=$DATA_DIR --job-dir=$TRAINING_DIR/$BUILD_ID --sync Set parallel/worker Replicas Count to 2 or more Set parallel/worker GPU Count to 1 Change parallel/ps Execution Command to: TF_CONFIG=$(tf_conf ps --chief_name master) python cifar10_main.py --num-gpus=$GPU_COUNT --train-steps=1000 --data-dir=$DATA_DIR --job-dir=$TRAINING_DIR/$BUILD_ID --sync Now we are ready to start distributed training. During execution the following process will be started: Two workers, one of them is chief One Parameter server Notes: Remove --sync options for asynchronous training, see Tensorflow documentation for more details. tf_conf command is always present in Kibernetika environment Usage of tf_conf command # tf_conf is basic script that helps define the environment for Tensorflow distributed training. usage: tf_conf [-h] [--worker WORKER] [--ps PS] {worker,ps,eval} [--chief_name CHIEF_NAME] positional arguments: {worker,ps,eval} Set executable role optional arguments: -h, --help show this help message and exit --worker WORKER Worker resource name --ps PS PS server resource name --chief_name CHIEF_NAME Name for the chief worker. 'chief' for newer Tensorflow version and 'master' for tf.contrib.learn Also, you could setup distributed configuration directly in your code: from mlboardclient import utils conf = utils.setup_tf_distributed(mode, worker_names='worker', ps_names='ps',chief_name='cheif') os.environ['TF_CONFIG'] = conf Notes: Please see link for more low levels details about distributed training on the Kibernetika platform. Please see horovod for using Horovod and OpenMPI for distributed training","title":"Train CIFAR-10 Model from scratch using Kibernetika.AI"},{"location":"how-to/cifar-10/#train-cifar-10-model-from-scratch-using-kibernetikaai","text":"This tutorial shows basic steps required to train CIFAR-10 model using original source code from https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator","title":"Train CIFAR-10 Model from scratch using Kibernetika.AI"},{"location":"how-to/cifar-10/#install-tensorflow-project","text":"Start creating a new project in your Workspace. More details Set Tensorflow version 1.9.0 Set source location to https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator Notes: In this case, https://github.com/tensorflow/models will be used as a Source repository and models/tutorials/image/cifar10_estimator as a subpath: Contents of this repository dir will be visible inside Jupyter or running Job under $SRC_DIR path, usually SRC_DIR is an alias for /norebooks/src","title":"Install Tensorflow project"},{"location":"how-to/cifar-10/#prepare-dataset","text":"First, we need to upload CIFAR-10 to Kibernetika. There are two options to upload data. Upload data to some project directory, for example to $DATA_DIR . Upload data to Kibernetika DataSet Catalog. The second option is preferable as it will allow you to track versions of your dataset and also use the dataset in other projects. Following steps required to upload dataset to catalog: Create new task upload-dataset with resource worker for dataset uploading. Inside the project, clone an existing task or create a new one from scratch. After creating the task we are ready to define the execution command for uploading. The CIFAR-10 project already has code for that. Our task definition looks like: Basically we defined the following parameters: Execution Directory: $SRC_DIR refers to the location our CIFAR-10 source code is Execution Command: mkdir /tmp/cifar-10 && python generate_cifar10_tfrecords.py --data-dir=/tmp/cifar-10 && cd /tmp/cifar-10 && kdataset push $WORKSPACE_NAME cifar-10:1.0.0 --create Ensure that Source is mounted to your task. Open \"Advanced\" section at the bottom of form and check option \"Default volume mapping\" or add needed volume manually. During running of the new task the following steps will be executed: Make temporary directory /tmp/cifar-10 Use generate_cifar10_tfrecords.py to upload dataset to /tmp/cifar-10 Change current directory /tmp/cifar-10 Push cifar-10 dataset to current workspace DataSet catalog (use environment variable $WORKSPACE_NAME for current workspace) as version 1.0.0. Option --create means create dataset if it doesn\u2019t exist. After execution upload-dataset we can refer our data directory to the created dataset. Change definition data volume in the Sources tab to point it to the newly created dataset: Notes: kdataset command is always present in Kibernetika environment you also could push dataset to catalog directly using python script: from mlboardclient.api import client mlboard = client.Client() mlboard.datasets.push( os.environ.get('WORKSPACE_NAME'), 'cifar-10', '1.0.0', '/tmp/cifar-10', create=True, )","title":"Prepare Dataset"},{"location":"how-to/cifar-10/#standard-train-model","text":"To start training, we need to configure resource worker in task standalone to train the model. Set execution directory to $SRC_DIR Set execution command to: python cifar10_main.py --num-gpus=$GPU_COUNT --train-steps=1000 --data-dir=$DATA_DIR --job-dir=$TRAINING_DIR/$BUILD_ID Set required GPU count in the Resources section Start task Notes: $TRAINING_DIR is an alias for preconfigured training directory, see Sources tab $BUILD_ID is an alias for sequential job id, every running job has a unique id $GPU_COUNT is an alias for number of GPU allocated for execution on one compute node You can see execution logs in the Jobs tab. Use Tensorboard tab to see your training progress. The result of model training is available under training directory $TRAINING_DIR/$BUILD_ID (usually /notebooks/training/1,2\u2026 ).","title":"Standard Train model"},{"location":"how-to/cifar-10/#distributed-training","text":"ATTENTION: CIFAR-10 Tensorflow original model is based on tf.contrib.learn which was deprecated since Tensorflow 1.7 and distributed configuration is not compatible with newer tensorflow version. We recommend migrating your code to Tensorflow Estimators. In the Kibernetika platform you can use distributed training for both old and new style models, see details below. First, we need to define resources that will be used for distributed training, e.g. workers and parameter servers. Change parallel/worker Execution Command to: TF_CONFIG=$(tf_conf worker --chief_name master) python cifar10_main.py --num-gpus=$GPU_COUNT --train-steps=1000 --data-dir=$DATA_DIR --job-dir=$TRAINING_DIR/$BUILD_ID --sync Set parallel/worker Replicas Count to 2 or more Set parallel/worker GPU Count to 1 Change parallel/ps Execution Command to: TF_CONFIG=$(tf_conf ps --chief_name master) python cifar10_main.py --num-gpus=$GPU_COUNT --train-steps=1000 --data-dir=$DATA_DIR --job-dir=$TRAINING_DIR/$BUILD_ID --sync Now we are ready to start distributed training. During execution the following process will be started: Two workers, one of them is chief One Parameter server Notes: Remove --sync options for asynchronous training, see Tensorflow documentation for more details. tf_conf command is always present in Kibernetika environment","title":"Distributed training"},{"location":"how-to/cifar-10/#usage-of-tf_conf-command","text":"tf_conf is basic script that helps define the environment for Tensorflow distributed training. usage: tf_conf [-h] [--worker WORKER] [--ps PS] {worker,ps,eval} [--chief_name CHIEF_NAME] positional arguments: {worker,ps,eval} Set executable role optional arguments: -h, --help show this help message and exit --worker WORKER Worker resource name --ps PS PS server resource name --chief_name CHIEF_NAME Name for the chief worker. 'chief' for newer Tensorflow version and 'master' for tf.contrib.learn Also, you could setup distributed configuration directly in your code: from mlboardclient import utils conf = utils.setup_tf_distributed(mode, worker_names='worker', ps_names='ps',chief_name='cheif') os.environ['TF_CONFIG'] = conf Notes: Please see link for more low levels details about distributed training on the Kibernetika platform. Please see horovod for using Horovod and OpenMPI for distributed training","title":"Usage of tf_conf command"},{"location":"how-to/object-detection-pets/","text":"Train model for object-detecion using object-detection API # Tensorflow object-detection training Running training # Object detection pets dataset contains: pets tensorflow record pets label map pretrained coco model (downloaded from here ) To perform training, install object-detection project on cluster using Kibernetika platform. More details During installation, make sure to connect object-detection-pets dataset and object-detection-code model. Then, it is ready to start training: run task named train . Training with current settings will take several hours. You can change train steps number: Adjust (or add) argument --num_steps and pass the desired steps number. However, while training is running we can start task eval : it takes last tensorflow training checkpoint and log some images with detections to tensorboard . As the model training progresses, task eval can be performed many times to see the detection correctness for the model. Export model # To export model, need to adjust some parameters for task export : Change the execution command as follows: Specify --train_checkpoint argument according to num steps in task train Specify --train_build_id argument according to build id (task number) of task train Specify --model-name argument according to desired model name ( object-detection-pets recommended) Specify --model-version argument according to desired model version Then run task export . It will export TensorFlow saved model to the Kibernetika catalog into the current workspace. When the task finishes, you will see the link to your model: Run serving, request and detection # There is a pre-trained object-detection-pets model which can be used for serving already. Or, if you have run the export model from above, that model is ready for serving too. Just follow the link you got in export task and you will see the model page. Once you are on the model page, click Serve near the model version description: Then click Serve in the appeared form.","title":"Train model for object-detecion using object-detection API"},{"location":"how-to/object-detection-pets/#train-model-for-object-detecion-using-object-detection-api","text":"Tensorflow object-detection training","title":"Train model for object-detecion using object-detection API"},{"location":"how-to/object-detection-pets/#running-training","text":"Object detection pets dataset contains: pets tensorflow record pets label map pretrained coco model (downloaded from here ) To perform training, install object-detection project on cluster using Kibernetika platform. More details During installation, make sure to connect object-detection-pets dataset and object-detection-code model. Then, it is ready to start training: run task named train . Training with current settings will take several hours. You can change train steps number: Adjust (or add) argument --num_steps and pass the desired steps number. However, while training is running we can start task eval : it takes last tensorflow training checkpoint and log some images with detections to tensorboard . As the model training progresses, task eval can be performed many times to see the detection correctness for the model.","title":"Running training"},{"location":"how-to/object-detection-pets/#export-model","text":"To export model, need to adjust some parameters for task export : Change the execution command as follows: Specify --train_checkpoint argument according to num steps in task train Specify --train_build_id argument according to build id (task number) of task train Specify --model-name argument according to desired model name ( object-detection-pets recommended) Specify --model-version argument according to desired model version Then run task export . It will export TensorFlow saved model to the Kibernetika catalog into the current workspace. When the task finishes, you will see the link to your model:","title":"Export model"},{"location":"how-to/object-detection-pets/#run-serving-request-and-detection","text":"There is a pre-trained object-detection-pets model which can be used for serving already. Or, if you have run the export model from above, that model is ready for serving too. Just follow the link you got in export task and you will see the model page. Once you are on the model page, click Serve near the model version description: Then click Serve in the appeared form.","title":"Run serving, request and detection"},{"location":"models/models/","text":"Models # Models looks like Datasets with additional data that provides models serving. Also models can be served. Many model's functions are the similar with datasets but also models have any differences. First of all models has additional description for serving suck as serving configuration and specification for UI form. Also all models has Status, it can be \"uploading\" for models that currently still exporting from task, \"ok\" for success and \"error\" if any error occurs on task exporting. Create model # Model can be created from scratch similarly to Dataset creation . Also model can be exported from Project as Job's result. It can be make from \"Export\" option in job's context menu. User can choose existing model and versions in current organization, or can set new model or new version of existing model - they'll be created automatically. After clicking \"Save\" job will be exported to model. Managing models and versions # User can update , delete and fork Models in the same way as Datasets , also user can create , clone and delete Model versions in the same way as Datasets versions. Differences are described below. Model and version configurations # Like the Datasets Models have base parameters Name, Identification, Keywords, etc... But also Models have description for serving configuration and specification described in section Model serving . Configuration described directly in Model is default and this configuration is inherited in all Model versions except those versions with specifically described configuration. Model's default configuration can be edited with \"Edit\" option in Model's context menu. Version's configuration can be set in \"Config\" tab for every version.","title":"Models Management"},{"location":"models/models/#models","text":"Models looks like Datasets with additional data that provides models serving. Also models can be served. Many model's functions are the similar with datasets but also models have any differences. First of all models has additional description for serving suck as serving configuration and specification for UI form. Also all models has Status, it can be \"uploading\" for models that currently still exporting from task, \"ok\" for success and \"error\" if any error occurs on task exporting.","title":"Models"},{"location":"models/models/#create-model","text":"Model can be created from scratch similarly to Dataset creation . Also model can be exported from Project as Job's result. It can be make from \"Export\" option in job's context menu. User can choose existing model and versions in current organization, or can set new model or new version of existing model - they'll be created automatically. After clicking \"Save\" job will be exported to model.","title":"Create model"},{"location":"models/models/#managing-models-and-versions","text":"User can update , delete and fork Models in the same way as Datasets , also user can create , clone and delete Model versions in the same way as Datasets versions. Differences are described below.","title":"Managing models and versions"},{"location":"models/models/#model-and-version-configurations","text":"Like the Datasets Models have base parameters Name, Identification, Keywords, etc... But also Models have description for serving configuration and specification described in section Model serving . Configuration described directly in Model is default and this configuration is inherited in all Model versions except those versions with specifically described configuration. Model's default configuration can be edited with \"Edit\" option in Model's context menu. Version's configuration can be set in \"Config\" tab for every version.","title":"Model and version configurations"},{"location":"projects/create-new-project/","text":"Project is the environment for development and lifecycle management of ML/AI application. You can find a lot of templates and pre-built Projects in the Kibernetika catalog. You can create new project using UI interface or using command line tool . Creating empty project from UI interface # Open your workspace and select projects tab. Press \"Create New Project\" button or use context menu button to create a new project and choose one of the template to use for your new Project. Set your project name. Valid name must be 63 characters or less and must begin and end with an lower case alphanumeric character ([a-z0-9]) with dashes (-) and lower case alphanumerics between. Make basic configuration of your project and select a Kibernetika Storage or a Cluster Storage available for your cluster. One more time verify the final configuration of the Project and press install button. Yore are almost done. Now you have completely configured environment to work with your ML/AI application. Please see Work With Project for additional documentations. Creating empty project using Kibernetika command line tool # Coming soon.","title":"Create New Project"},{"location":"projects/create-new-project/#creating-empty-project-from-ui-interface","text":"Open your workspace and select projects tab. Press \"Create New Project\" button or use context menu button to create a new project and choose one of the template to use for your new Project. Set your project name. Valid name must be 63 characters or less and must begin and end with an lower case alphanumeric character ([a-z0-9]) with dashes (-) and lower case alphanumerics between. Make basic configuration of your project and select a Kibernetika Storage or a Cluster Storage available for your cluster. One more time verify the final configuration of the Project and press install button. Yore are almost done. Now you have completely configured environment to work with your ML/AI application. Please see Work With Project for additional documentations.","title":"Creating empty project from UI interface"},{"location":"projects/create-new-project/#creating-empty-project-using-kibernetika-command-line-tool","text":"Coming soon.","title":"Creating empty project using Kibernetika command line tool"},{"location":"projects/working-with-projects/","text":"Project Summary # The project summary ('SUMMARY' tab) contains descriptive information and additional materials for your project. You can enter any information here that you want to go with your project. Use the edit (pen) symbol at bottom right to start editing content. The supported format for this content is Markdown Task management # The typical way to execute your AI application is to run it with tasks ('TASKS' tab). Thus, every project contains a number of tasks that the user can schedule as required. All base project templates and tutorials from the Kibernetika catalog already have some preconfigured tasks. You can modify those tasks, or remove or create new ones. Every task consists of one or more execution entities (like processes or threads) that are called 'resources'. Distributed processing typically requires multiple replicas of resources running in parallel. A resource may have any number of replicas. Each replica is executed inside the container during task execution, and the replicas can communicate with each other through the TCP or UDP protocols. In the figures above, the tasks and resources are shown in the vertical pane on the left. The upper-level names are the tasks (prepare-data, standalone, parallel, export, workflow, etc.) and the indented names are the resources within each task (upload, worker, ps, etc.) Each resource has a form on the right of the task pane, to control the execution. The following variables are available to control resource execution and replication in an execution container: Variable Purpose Example values GPU_COUNT Number of GPU instances available for each replica 0,1, ...,n PYTHONPATH Location of the python library $LIB_DIR:.... REPLICA_INDEX Index of the current resource replica. Required for distributed training 1,2,... upper_case({ResourceName})_NODES comma separated list of dns addresses resource replicas T1-R1-0-0.R1-0,T1-R1-0-1.R1-0,... (where T1 task name,R1 resource name) BUILD_ID Id of current task run 1,2...,n upper_case({SourceName})_DIR Mount point for Source with name 'SourceName' /workspace/src The following is a list of parameters used to specify task execution: Field Purpose Example values Execution directory Directory to run user commands in $SRC_DIR Timeout Time to wait for a compute resource 300 Execution command Command to start the compute process python styles.py --job_name=worker --train_dir=$TRAINING_DIR/$BUILD_ID --task_index=$REPLICA_INDEX --ps_hosts=$PS_NODES --worker_hosts=$WORKER_NODES Resources Minimum and maximum compute resource requirements Requires at least CPU=100m, Memory=62Mi, but no more than CPU=4000m, Memory=8Gi and GPU=1 for each resource replica Environment variables User defined environment variables MY_VARIABLE = value Images Container images that will be used for cpu and gpu (tensorflow/tensorflow:1.2.0,tensorflow/tensorflow:1.2.0-gpu) Default volume mapping Useful to mount all sources to container as they are defined in the sources true or false Default mount path Fixed prefix to the mount points for all sources Volumes Custom sources to be mounted to the container (data, subfolder, /mynewfolder/andsubfolder) Node Allocator For public clouds only. Template to allocate new compute resources on the public cloud if there aren't resources available Template name from your cluster configuration Working with sources # A project requires data sources to be configured for training data, model source code, results and checkpoints. You can configure all data sources in the \"SOURCES\" tab and mount it to different Tasks or to specific Project components like Jupyter or Tensorboard. We support the following data source types: GIT: Git repository. You can connect you GitHub, GitLab or BitBucket account or use any other git server with your Kibernetika account. NFS: External network file system that supports the NFS protocol. Cluster Storage: Storage attached to your cluster by your administrator (in case of a local cluster), or provided by the Kibernetika service. See Cluster Storage for details. S3 Bucket: Storage that uses your S3 bucket as a data source. When you create a project using either the recommended templates, or a tutorial or sample from the Kibernetika catalog, the required sources are automatically created for you. To add new storage or edit the existing entries, click on the \"SOURCES\" tab in the Project screen. Configuration: Name: Data source name. Valid names are 63 characters or less and must begin and end with an lower case alphanumeric character ([a-z0-9]) with dashes (-) and lower case alphanumeric characters in between. Sub Path: Optional field. Path to mount the directory inside attached file system. Mount Path: Path inside the Project. This is a required field. Type: Choose one of the data source types. This is a required field. Git data source # To connect a git data source you need to specify the following fields: Repository: Git repository path, like https://github.com/Kibernetika-catalog/tensorflow. You can use the \"Expand\" button to specify repository details if you have connected a git account in the user settings . Sub Path - by default content of git repository will be visible in the Project under \"MountPath/RepoName\". By specifying Sub Path you can change this behavior. For example if your repository has \"src\" folder and you want to mount its content to \"MountPath/\" you should set \"Sub Path\" to \"RepositoryName/src\". Account - Account must be set for private repository if you want to be able to commit changes. You need to provide secret that hold user private data, like access key or deploy key. See service account management Note that the \"Account\" field is required only if you have a private Github repository. Unless you have such a repository configured in the user settings, this field will remain unavailable (set to \"without account\"). NFS data source # This is an external network file system that support the NFS protocol. To connect a NFS source type you need to specify the following fields: Server: The IP address of your NFS server. Path: NFS internal path. Mount Path: The file path at which you want the data available. Sub Path: Path to data inside the NFS volume. For example if your want to mount the directory \"/mypath/data\" from the NFS server to the directory \"/MountPath/data\", you should set the mount path to \"/MountPath/data\" and the sub path to \"/mypath/data\" Cluster Storage # This is storage attached to a shared cluster or to a cluster from your infrastructure. If you do not have cluster storage configured, contact your administrator or support. To connect cluster storage you must specify one of the available cluster storages. See Cluster Storage and Kibernetika Storage for information about creating and managing this data source type. ATTENTION : The following rules are applied to the 'Sub Path' field: If Sub Path begins with the '/' symbol then the data will be visible for all projects in your organization and you can share this data to other projects belonging to the same organization. If the Sub Path begins with '/shared/' then data will be visible to everyone who has access to the same storage volume, even from another organization. Otherwise data will be visible only inside your project. S3 Bucket Storage # This is storage that allows working with S3 bucket data. To connect a S3 source type you need to specify the following fields: Server: The address of your S3 server. Leave it empty for Amazon S3. Bucket: The bucket name. Account: Secret that holds the user's private credentials that will be used for the S3 connection. See service account management History # This tab shows a list of all resource instances run, with some details about them. Jobs # Coming soon. Metrics # This tab shows various metrics of used resources in current project. Resources are grouped by user's tasks, project UIX components (Jupyter, Tensorboard, etc...) and servings. UIX components and servings metrics can be displayed for user selected time period (from last 5 minutes to 1 last day). Tasks metrics can be shown for whole task execution period (but not longer than 1 week) with the user specified interval (from 10 seconds to 1 hour). The following metrics are available: CPU usage in percents Memory usage in Megabytes GPU usage in percents GPU memory in Megabytes GPU power draw in Watts GPU temperature in Celsius Status # Status tab shows actual containers statuses: Also it's available to see events on problem nodes, in following example case we can see problems with repository mounting: Simple form # Project owner can configure simple form in any project to automate dialog with user. Simple form is available in \"Simply\" tab. Form can be described in YAML. For example, let's take a look to this form: User can fill all form fields and click \"Build\", and special task will be started with specified form values. This form is described by the following YAML: - name: model_name label: Export to model type: string value: object-detection width: 50 - name: model_version label: Model version type: string value: 1.0.0 width: 50 - name: num_steps label: Train steps quantity type: int value: 1000 - label: Resize type: group elements: - name: resize_min_dimension label: Minimal resize dimension type: int value: 600 width: 50 - name: resize_max_dimension label: Miximal resize dimension type: int value: 1024 width: 50 - name: resize_fixed_width label: Fixed resize width type: int width: 50 - name: resize_fixed_height label: Fixed resize height type: int width: 50 - name: grid_scales label: Grid generator scales type: list value: - 0.25 - 0.5 - 1 - 2 width: 50 listType: float - name: grid_aspect_ratios label: Grid generator aspect ratios type: list value: - 0.5 - 1 - 2 width: 50 listType: float Each form's field is described with the following parameters: name - parameter's name, required for all fields (but not for groups) label - parameter's label, uses name if not set type - parameter's type, required for all fields and for group, available values see below value - default value width - field's width in percents (default 100) listType - field's type, required for type: list , available values as for type excepts list and group elements - fields in group, required for type: group Available field types: int , int8 , int16 , int32 , int64 - integer value uint8 , uint16 - unsigned integer values float , double - floating point values byte , bytes - upload file field string , strings - string value (text field) list - multiple values group - group of fields Simple form execution template can be edited in \"Edit template\" context menu: Execution template describes YAML with task's name and resources, for example: name: example resources: - workDir: $SRC_DIR command: python example.py --parameter_name {{ .parameter_value }} default_volume_mapping: true images: cpu: kuberlab/tensorflow:cpu-36-1.7.0-full gpu: kuberlab/tensorflow:gpu-36-1.7.0-full name: worker replicas: 1 resources: limits: cpu: \"1\" memory: 8Gi requests: cpu: 100m memory: 64Mi restartPolicy: Never Integrate Project to your workflow engine # Coming soon.","title":"Working With Projects"},{"location":"projects/working-with-projects/#project-summary","text":"The project summary ('SUMMARY' tab) contains descriptive information and additional materials for your project. You can enter any information here that you want to go with your project. Use the edit (pen) symbol at bottom right to start editing content. The supported format for this content is Markdown","title":"Project Summary"},{"location":"projects/working-with-projects/#task-management","text":"The typical way to execute your AI application is to run it with tasks ('TASKS' tab). Thus, every project contains a number of tasks that the user can schedule as required. All base project templates and tutorials from the Kibernetika catalog already have some preconfigured tasks. You can modify those tasks, or remove or create new ones. Every task consists of one or more execution entities (like processes or threads) that are called 'resources'. Distributed processing typically requires multiple replicas of resources running in parallel. A resource may have any number of replicas. Each replica is executed inside the container during task execution, and the replicas can communicate with each other through the TCP or UDP protocols. In the figures above, the tasks and resources are shown in the vertical pane on the left. The upper-level names are the tasks (prepare-data, standalone, parallel, export, workflow, etc.) and the indented names are the resources within each task (upload, worker, ps, etc.) Each resource has a form on the right of the task pane, to control the execution. The following variables are available to control resource execution and replication in an execution container: Variable Purpose Example values GPU_COUNT Number of GPU instances available for each replica 0,1, ...,n PYTHONPATH Location of the python library $LIB_DIR:.... REPLICA_INDEX Index of the current resource replica. Required for distributed training 1,2,... upper_case({ResourceName})_NODES comma separated list of dns addresses resource replicas T1-R1-0-0.R1-0,T1-R1-0-1.R1-0,... (where T1 task name,R1 resource name) BUILD_ID Id of current task run 1,2...,n upper_case({SourceName})_DIR Mount point for Source with name 'SourceName' /workspace/src The following is a list of parameters used to specify task execution: Field Purpose Example values Execution directory Directory to run user commands in $SRC_DIR Timeout Time to wait for a compute resource 300 Execution command Command to start the compute process python styles.py --job_name=worker --train_dir=$TRAINING_DIR/$BUILD_ID --task_index=$REPLICA_INDEX --ps_hosts=$PS_NODES --worker_hosts=$WORKER_NODES Resources Minimum and maximum compute resource requirements Requires at least CPU=100m, Memory=62Mi, but no more than CPU=4000m, Memory=8Gi and GPU=1 for each resource replica Environment variables User defined environment variables MY_VARIABLE = value Images Container images that will be used for cpu and gpu (tensorflow/tensorflow:1.2.0,tensorflow/tensorflow:1.2.0-gpu) Default volume mapping Useful to mount all sources to container as they are defined in the sources true or false Default mount path Fixed prefix to the mount points for all sources Volumes Custom sources to be mounted to the container (data, subfolder, /mynewfolder/andsubfolder) Node Allocator For public clouds only. Template to allocate new compute resources on the public cloud if there aren't resources available Template name from your cluster configuration","title":"Task management"},{"location":"projects/working-with-projects/#working-with-sources","text":"A project requires data sources to be configured for training data, model source code, results and checkpoints. You can configure all data sources in the \"SOURCES\" tab and mount it to different Tasks or to specific Project components like Jupyter or Tensorboard. We support the following data source types: GIT: Git repository. You can connect you GitHub, GitLab or BitBucket account or use any other git server with your Kibernetika account. NFS: External network file system that supports the NFS protocol. Cluster Storage: Storage attached to your cluster by your administrator (in case of a local cluster), or provided by the Kibernetika service. See Cluster Storage for details. S3 Bucket: Storage that uses your S3 bucket as a data source. When you create a project using either the recommended templates, or a tutorial or sample from the Kibernetika catalog, the required sources are automatically created for you. To add new storage or edit the existing entries, click on the \"SOURCES\" tab in the Project screen. Configuration: Name: Data source name. Valid names are 63 characters or less and must begin and end with an lower case alphanumeric character ([a-z0-9]) with dashes (-) and lower case alphanumeric characters in between. Sub Path: Optional field. Path to mount the directory inside attached file system. Mount Path: Path inside the Project. This is a required field. Type: Choose one of the data source types. This is a required field.","title":"Working with sources"},{"location":"projects/working-with-projects/#git-data-source","text":"To connect a git data source you need to specify the following fields: Repository: Git repository path, like https://github.com/Kibernetika-catalog/tensorflow. You can use the \"Expand\" button to specify repository details if you have connected a git account in the user settings . Sub Path - by default content of git repository will be visible in the Project under \"MountPath/RepoName\". By specifying Sub Path you can change this behavior. For example if your repository has \"src\" folder and you want to mount its content to \"MountPath/\" you should set \"Sub Path\" to \"RepositoryName/src\". Account - Account must be set for private repository if you want to be able to commit changes. You need to provide secret that hold user private data, like access key or deploy key. See service account management Note that the \"Account\" field is required only if you have a private Github repository. Unless you have such a repository configured in the user settings, this field will remain unavailable (set to \"without account\").","title":"Git data source"},{"location":"projects/working-with-projects/#cluster-storage","text":"This is storage attached to a shared cluster or to a cluster from your infrastructure. If you do not have cluster storage configured, contact your administrator or support. To connect cluster storage you must specify one of the available cluster storages. See Cluster Storage and Kibernetika Storage for information about creating and managing this data source type. ATTENTION : The following rules are applied to the 'Sub Path' field: If Sub Path begins with the '/' symbol then the data will be visible for all projects in your organization and you can share this data to other projects belonging to the same organization. If the Sub Path begins with '/shared/' then data will be visible to everyone who has access to the same storage volume, even from another organization. Otherwise data will be visible only inside your project.","title":"Cluster Storage"},{"location":"projects/working-with-projects/#s3-bucket-storage","text":"This is storage that allows working with S3 bucket data. To connect a S3 source type you need to specify the following fields: Server: The address of your S3 server. Leave it empty for Amazon S3. Bucket: The bucket name. Account: Secret that holds the user's private credentials that will be used for the S3 connection. See service account management","title":"S3 Bucket Storage"},{"location":"projects/working-with-projects/#history","text":"This tab shows a list of all resource instances run, with some details about them.","title":"History"},{"location":"projects/working-with-projects/#jobs","text":"Coming soon.","title":"Jobs"},{"location":"projects/working-with-projects/#metrics","text":"This tab shows various metrics of used resources in current project. Resources are grouped by user's tasks, project UIX components (Jupyter, Tensorboard, etc...) and servings. UIX components and servings metrics can be displayed for user selected time period (from last 5 minutes to 1 last day). Tasks metrics can be shown for whole task execution period (but not longer than 1 week) with the user specified interval (from 10 seconds to 1 hour). The following metrics are available: CPU usage in percents Memory usage in Megabytes GPU usage in percents GPU memory in Megabytes GPU power draw in Watts GPU temperature in Celsius","title":"Metrics"},{"location":"projects/working-with-projects/#status","text":"Status tab shows actual containers statuses: Also it's available to see events on problem nodes, in following example case we can see problems with repository mounting:","title":"Status"},{"location":"projects/working-with-projects/#simple-form","text":"Project owner can configure simple form in any project to automate dialog with user. Simple form is available in \"Simply\" tab. Form can be described in YAML. For example, let's take a look to this form: User can fill all form fields and click \"Build\", and special task will be started with specified form values. This form is described by the following YAML: - name: model_name label: Export to model type: string value: object-detection width: 50 - name: model_version label: Model version type: string value: 1.0.0 width: 50 - name: num_steps label: Train steps quantity type: int value: 1000 - label: Resize type: group elements: - name: resize_min_dimension label: Minimal resize dimension type: int value: 600 width: 50 - name: resize_max_dimension label: Miximal resize dimension type: int value: 1024 width: 50 - name: resize_fixed_width label: Fixed resize width type: int width: 50 - name: resize_fixed_height label: Fixed resize height type: int width: 50 - name: grid_scales label: Grid generator scales type: list value: - 0.25 - 0.5 - 1 - 2 width: 50 listType: float - name: grid_aspect_ratios label: Grid generator aspect ratios type: list value: - 0.5 - 1 - 2 width: 50 listType: float Each form's field is described with the following parameters: name - parameter's name, required for all fields (but not for groups) label - parameter's label, uses name if not set type - parameter's type, required for all fields and for group, available values see below value - default value width - field's width in percents (default 100) listType - field's type, required for type: list , available values as for type excepts list and group elements - fields in group, required for type: group Available field types: int , int8 , int16 , int32 , int64 - integer value uint8 , uint16 - unsigned integer values float , double - floating point values byte , bytes - upload file field string , strings - string value (text field) list - multiple values group - group of fields Simple form execution template can be edited in \"Edit template\" context menu: Execution template describes YAML with task's name and resources, for example: name: example resources: - workDir: $SRC_DIR command: python example.py --parameter_name {{ .parameter_value }} default_volume_mapping: true images: cpu: kuberlab/tensorflow:cpu-36-1.7.0-full gpu: kuberlab/tensorflow:gpu-36-1.7.0-full name: worker replicas: 1 resources: limits: cpu: \"1\" memory: 8Gi requests: cpu: 100m memory: 64Mi restartPolicy: Never","title":"Simple form"},{"location":"projects/working-with-projects/#integrate-project-to-your-workflow-engine","text":"Coming soon.","title":"Integrate Project to your workflow engine"},{"location":"resources/kibernetika-storage/","text":"Create new Kibernetika Storage # Kibernetika Storage is essentially a persistent volume which can be used for any data in a project. To create a new Kibernetika Storage go to -> Resources tab and scroll down to Kibernetika storage . Click Add, then specify storage name, size in GB and connected cluster (and optionally description): Delete Kibernetika Storage # To delete Kibernetika Storage , just simply click delete in storage options:","title":"Storage"},{"location":"resources/kibernetika-storage/#create-new-kibernetika-storage","text":"Kibernetika Storage is essentially a persistent volume which can be used for any data in a project. To create a new Kibernetika Storage go to -> Resources tab and scroll down to Kibernetika storage . Click Add, then specify storage name, size in GB and connected cluster (and optionally description):","title":"Create new Kibernetika Storage"},{"location":"resources/kibernetika-storage/#delete-kibernetika-storage","text":"To delete Kibernetika Storage , just simply click delete in storage options:","title":"Delete Kibernetika Storage"},{"location":"serving/kibernetika-serving/","text":"kibernetika-serving (ex. kuberlab-serving) # This is a document describing the possibilities and parameters of kibernetika-serving tool. What is it? # kibernetika-serving tool is a generic machine-learning model runner. Basically, it starts the gRPC server and receives google protobuf messages (just like tensorflow_model_server does) and optionally, it can start HTTP proxy to that gRPC server, so the requests to the server become much more easier. It supports the following machine learning frameworks and formats: TensorFlow ONNX (via Intel nGraph) Intel OpenVINO PyTorch Also, it can be run without model and all required logic may be in the process hooks (see hooks section below), for that need to take null driver. kibernetika-serving tool is available in serving containers: kuberlab/serving:latest (basic image, doesn't include OpenVINO support) kuberlab/serving/latest-gpu (includes GPU-related stuff) kuberlab/serving:latest-openvino (includes OpenVINO) CLI interface # Once you have an access to kibernetika-serving executable, you are ready to use it. Let's see the options and flags which can be provided during start. Note : There is an alias kserving for kibernetika-serving . usage: kibernetika-serving [-h] [--driver DRIVER] --model-path MODEL_PATH [--hooks HOOKS] [--option OPTION] [--port <int>] [--http-enable] [--http-server-command HTTP_SERVER_COMMAND] [--http-port HTTP_PORT] optional arguments: -h, --help : show this help message and exit --driver DRIVER : Driver to use in ML serving server. --model-path MODEL_PATH : Path to model file or directory. --hooks HOOKS : Hooks python file containing preprocess(inputs) and postprocess(outputs) functions. --option OPTION, -o OPTION : Additional options specific to driver. format: -o option_name=option_value --port : Port on which server will listen --http-enable : Enables HTTP proxy for gRPC server --http-server-command HTTP_SERVER_COMMAND : Command for running http tfservable-proxy --http-port HTTP_PORT : Port for http server Drivers # This section describes the list of available backends (drivers) and their specific options. TensorFlow # Driver name used in options: tensorflow . Options which are used in TensorFlow driver (may be provided via -o option_name=option_value ): model_signature . Used if --model-path is a saved-model-dir. If so, driver extracts provided model_signature from saved_model.pb . Default value is serving_default (default constant in tensorflow package). Example: -o model_signature=transform inputs : Used only if --model-path is a .pb graph file. Service will be using the provided tensor names as an input. Example: -o inputs=input,prob1 outputs : Used only if --model-path is a .pb graph file. Service will be using the provided tensor names as an output. Example: -o outputs=output,embeddings Values accessible from processing hooks: graph : TensorFlow graph sess : TensorFlow session model_inputs : dict of tensor inputs, tensor name as a key and tensor as a value model_outputs : list of output tensor names driver : TensorFlowDriver object. PyTorch # Driver name used in options: pytorch . Options which are used in PyTorch driver (may be provided via -o option_name=option_value ): model_class . Required . It is an import path to PyTorch Net class. Example: -o model_class=package.module:NetClass Values accessible from processing hooks: model : PyTorch model object model_class : PyTorch model class driver : PyTorchDriver object. Intel OpenVINO # Driver name used in options: openvino . Options which are used in Intel OpenVINO driver (may be provided via -o option_name=option_value ): model_weights : Path to weights ( .bin ) file. May be used in case weights file is in another location with .xml file device : The device which should be used for computation. Multiple devices can be provided so in this case HETERO plugin will be used. Possible values: CPU , MYRIAD , FPGA . Examples: -o device=CPU , -o device=MYRIAD , -o device=CPU,MYRIAD . flexible_batch_size : Use variable first dimension in input data. In this case the network will be executed multiple times. For example, if your network receive input shape (1, 28, 28) and outputs (1, 10), then flexible_batch_size enables the possibility to pass (N, 28, 28) as an input. Then the serving output will be (N, 10) accordingly to each input request. Example: -o flexible_batch_size=true Values accessible from processing hooks: exec_net : Executable Network object model_inputs : Input model dict: input name -> input shape model_outputs : Output name list plugin : Plugin object (may be used in order to load more networks in hooks on the same device) driver : IntelOpenVINODriver object ONNX (Open Neural Network Exchange) # Driver name used in options: onnx . Options which are used in ONNX driver (may be provided via -o option_name=option_value ): runtime_backend : Runtime backend used for initilizing backend. Possible values: CPU , GPU , INTERPRETER , ARGON , NNP . Default value - CPU . Null driver # Driver name used in options: null . This driver is essentially needed only for using in conjunction with hooks. It provides the possibility to create your own serving logic without a model at all. The driver itself just returns what it received as an input. No other specific options are provided. Hooks # This section describes the possible ways of writing serving hooks and their capabilities. Hooks file structure # Basic hook functions set: init_hook(**params) : function-initializer. params is a dict containing all the options passed during start with -o flags and also containing some additional parameters like model_path . preprocess(inputs, ctx, **kwargs) : Hook which executes before model inference. Possible function declarations: preprocess(inputs) ( ctx and kwargs unused), preprocess(inputs, ctx) ( kwargs unused), preprocess(inputs, **kwargs) ( ctx unused). postprocess(outputs, ctx, **kwargs) : Hook which executes after model inference. Possible function declarations: postprocess(outputs) ( ctx and kwargs unused), postprocess(outputs, ctx) ( kwargs unused), postprocess(outputs, **kwargs) ( ctx unused). Arguments in pre/postprocess hooks: inputs - dict containing input numpy arrays, e.g. {'input-name': <numpy array>} outputs - dict containing output numpy arrays, e.g. {'output-name': <numpy array>} ctx - gRPC context object. It can be used to pass data from preprocess hook to postprocess hook. Example: we can set some attribute to ctx object in preprocess hook and then read it in postprocess hook. **kwargs - key-value arguments from the driver. Each driver has a different set of kwargs arguments which passed to the hook (see driver section for the details) Hook example Here is a hook example which just logs the fact of calling itself. hooks.py import logging LOG = logging.getLogger(__name__) def init_hook(**params): LOG.info(\"Got params:\") LOG.info(params) def preprocess(inputs): \"\"\"Does processing inputs before the model inference. For example, here we can do checks, change shape and other ops. :param inputs: dict containing input numpy arrays: {'input-name': <numpy array>} :type inputs: dict :return: processed inputs dict \"\"\" LOG.info('Running preprocess...') return inputs def postprocess(outputs): \"\"\"Does processing outputs after the model inference. :param outputs: dict containing input numpy arrays: {'input-name': <numpy array>} :type outputs: dict :return: processed outputs dict \"\"\" LOG.info('Running postprocess...') return outputs Launching multiple models # For launching more than 1 model, need to specify appropriate hooks for pre- and postprocessing in order to connect current model output and a next model input. At the end - all the models are lining up in one pipeline: pre-hook1 -> model1 -> post-hook1 -> pre-hook2 -> model2 -> post-hook2 Therefore, hook file requires more than one pre- and postprocessing functions. For doing that, preprocess and postprocess objects in the hooks file must be a list of functions with model-number length. None in the list may mean that there is no hook in this place. Multiple models hooks file example # For the examples below, kibernetika-serving may be launched using the following command: kibernetika-serving --driver null --model-path any --driver null --model-path any2 Note: The example should work with null driver but you might be interested in changing --driver and --model-path params for using your own driver and model. The example merely shows a concept itself. import logging LOG = logging.getLogger(__name__) def log(func): def decorator(*args): LOG.info('Running %s...' % func.__name__) return func(*args) return decorator def init_hook(**params): LOG.info(\"Got params:\") LOG.info(params) @log def preprocess1(inputs): return inputs @log def preprocess2(inputs): return inputs @log def postprocess1(outputs): return outputs @log def postprocess2(outputs): return outputs preprocess = [preprocess1, preprocess2] postprocess = [postprocess1, postprocess2] Partially implemented hooks: import logging LOG = logging.getLogger(__name__) def log(func): def decorator(*args): LOG.info('Running %s...' % func.__name__) return func(*args) return decorator def init_hook(**params): LOG.info(\"Got params:\") LOG.info(params) @log def preprocess1(inputs): return inputs @log def postprocess2(outputs): return outputs preprocess = [preprocess1, None] postprocess = [None, postprocess2] Ignore (skip) model inference # In some cases it needs to skip model inference (more useful when using multiple models). For example, first model detects faces and the second one - emotions on this faces. There is a case where there are no faces on image at all - therefore emotion detection doesn't make sense anymore because it requires face boxes. Example : def preprocess1(inputs): return inputs def postprocess1(outputs, ctx): # if length of face boxes is 0 -> skip_next is True else False ctx.skip_next = len(outputs.get('face-boxes', [])) == 0 returnt outputs def preprocess2(inputs, ctx): if ctx.skip_next: inputs['ml-serving-ignore'] = True return inputs def postprocess2(outputs, ctx): # Was skipped? if ctx.skip_next: # Nothing to return in case if skipped return {} return outputs preprocess = [preprocess1, preprocess2] postprocess = [postprocess1, postprocess2]","title":"kibernetika-serving"},{"location":"serving/kibernetika-serving/#kibernetika-serving-ex-kuberlab-serving","text":"This is a document describing the possibilities and parameters of kibernetika-serving tool.","title":"kibernetika-serving (ex. kuberlab-serving)"},{"location":"serving/kibernetika-serving/#what-is-it","text":"kibernetika-serving tool is a generic machine-learning model runner. Basically, it starts the gRPC server and receives google protobuf messages (just like tensorflow_model_server does) and optionally, it can start HTTP proxy to that gRPC server, so the requests to the server become much more easier. It supports the following machine learning frameworks and formats: TensorFlow ONNX (via Intel nGraph) Intel OpenVINO PyTorch Also, it can be run without model and all required logic may be in the process hooks (see hooks section below), for that need to take null driver. kibernetika-serving tool is available in serving containers: kuberlab/serving:latest (basic image, doesn't include OpenVINO support) kuberlab/serving/latest-gpu (includes GPU-related stuff) kuberlab/serving:latest-openvino (includes OpenVINO)","title":"What is it?"},{"location":"serving/kibernetika-serving/#cli-interface","text":"Once you have an access to kibernetika-serving executable, you are ready to use it. Let's see the options and flags which can be provided during start. Note : There is an alias kserving for kibernetika-serving . usage: kibernetika-serving [-h] [--driver DRIVER] --model-path MODEL_PATH [--hooks HOOKS] [--option OPTION] [--port <int>] [--http-enable] [--http-server-command HTTP_SERVER_COMMAND] [--http-port HTTP_PORT] optional arguments: -h, --help : show this help message and exit --driver DRIVER : Driver to use in ML serving server. --model-path MODEL_PATH : Path to model file or directory. --hooks HOOKS : Hooks python file containing preprocess(inputs) and postprocess(outputs) functions. --option OPTION, -o OPTION : Additional options specific to driver. format: -o option_name=option_value --port : Port on which server will listen --http-enable : Enables HTTP proxy for gRPC server --http-server-command HTTP_SERVER_COMMAND : Command for running http tfservable-proxy --http-port HTTP_PORT : Port for http server","title":"CLI interface"},{"location":"serving/kibernetika-serving/#drivers","text":"This section describes the list of available backends (drivers) and their specific options.","title":"Drivers"},{"location":"serving/kibernetika-serving/#tensorflow","text":"Driver name used in options: tensorflow . Options which are used in TensorFlow driver (may be provided via -o option_name=option_value ): model_signature . Used if --model-path is a saved-model-dir. If so, driver extracts provided model_signature from saved_model.pb . Default value is serving_default (default constant in tensorflow package). Example: -o model_signature=transform inputs : Used only if --model-path is a .pb graph file. Service will be using the provided tensor names as an input. Example: -o inputs=input,prob1 outputs : Used only if --model-path is a .pb graph file. Service will be using the provided tensor names as an output. Example: -o outputs=output,embeddings Values accessible from processing hooks: graph : TensorFlow graph sess : TensorFlow session model_inputs : dict of tensor inputs, tensor name as a key and tensor as a value model_outputs : list of output tensor names driver : TensorFlowDriver object.","title":"TensorFlow"},{"location":"serving/kibernetika-serving/#pytorch","text":"Driver name used in options: pytorch . Options which are used in PyTorch driver (may be provided via -o option_name=option_value ): model_class . Required . It is an import path to PyTorch Net class. Example: -o model_class=package.module:NetClass Values accessible from processing hooks: model : PyTorch model object model_class : PyTorch model class driver : PyTorchDriver object.","title":"PyTorch"},{"location":"serving/kibernetika-serving/#intel-openvino","text":"Driver name used in options: openvino . Options which are used in Intel OpenVINO driver (may be provided via -o option_name=option_value ): model_weights : Path to weights ( .bin ) file. May be used in case weights file is in another location with .xml file device : The device which should be used for computation. Multiple devices can be provided so in this case HETERO plugin will be used. Possible values: CPU , MYRIAD , FPGA . Examples: -o device=CPU , -o device=MYRIAD , -o device=CPU,MYRIAD . flexible_batch_size : Use variable first dimension in input data. In this case the network will be executed multiple times. For example, if your network receive input shape (1, 28, 28) and outputs (1, 10), then flexible_batch_size enables the possibility to pass (N, 28, 28) as an input. Then the serving output will be (N, 10) accordingly to each input request. Example: -o flexible_batch_size=true Values accessible from processing hooks: exec_net : Executable Network object model_inputs : Input model dict: input name -> input shape model_outputs : Output name list plugin : Plugin object (may be used in order to load more networks in hooks on the same device) driver : IntelOpenVINODriver object","title":"Intel OpenVINO"},{"location":"serving/kibernetika-serving/#onnx-open-neural-network-exchange","text":"Driver name used in options: onnx . Options which are used in ONNX driver (may be provided via -o option_name=option_value ): runtime_backend : Runtime backend used for initilizing backend. Possible values: CPU , GPU , INTERPRETER , ARGON , NNP . Default value - CPU .","title":"ONNX (Open Neural Network Exchange)"},{"location":"serving/kibernetika-serving/#null-driver","text":"Driver name used in options: null . This driver is essentially needed only for using in conjunction with hooks. It provides the possibility to create your own serving logic without a model at all. The driver itself just returns what it received as an input. No other specific options are provided.","title":"Null driver"},{"location":"serving/kibernetika-serving/#hooks","text":"This section describes the possible ways of writing serving hooks and their capabilities.","title":"Hooks"},{"location":"serving/kibernetika-serving/#hooks-file-structure","text":"Basic hook functions set: init_hook(**params) : function-initializer. params is a dict containing all the options passed during start with -o flags and also containing some additional parameters like model_path . preprocess(inputs, ctx, **kwargs) : Hook which executes before model inference. Possible function declarations: preprocess(inputs) ( ctx and kwargs unused), preprocess(inputs, ctx) ( kwargs unused), preprocess(inputs, **kwargs) ( ctx unused). postprocess(outputs, ctx, **kwargs) : Hook which executes after model inference. Possible function declarations: postprocess(outputs) ( ctx and kwargs unused), postprocess(outputs, ctx) ( kwargs unused), postprocess(outputs, **kwargs) ( ctx unused). Arguments in pre/postprocess hooks: inputs - dict containing input numpy arrays, e.g. {'input-name': <numpy array>} outputs - dict containing output numpy arrays, e.g. {'output-name': <numpy array>} ctx - gRPC context object. It can be used to pass data from preprocess hook to postprocess hook. Example: we can set some attribute to ctx object in preprocess hook and then read it in postprocess hook. **kwargs - key-value arguments from the driver. Each driver has a different set of kwargs arguments which passed to the hook (see driver section for the details) Hook example Here is a hook example which just logs the fact of calling itself. hooks.py import logging LOG = logging.getLogger(__name__) def init_hook(**params): LOG.info(\"Got params:\") LOG.info(params) def preprocess(inputs): \"\"\"Does processing inputs before the model inference. For example, here we can do checks, change shape and other ops. :param inputs: dict containing input numpy arrays: {'input-name': <numpy array>} :type inputs: dict :return: processed inputs dict \"\"\" LOG.info('Running preprocess...') return inputs def postprocess(outputs): \"\"\"Does processing outputs after the model inference. :param outputs: dict containing input numpy arrays: {'input-name': <numpy array>} :type outputs: dict :return: processed outputs dict \"\"\" LOG.info('Running postprocess...') return outputs","title":"Hooks file structure"},{"location":"serving/kibernetika-serving/#launching-multiple-models","text":"For launching more than 1 model, need to specify appropriate hooks for pre- and postprocessing in order to connect current model output and a next model input. At the end - all the models are lining up in one pipeline: pre-hook1 -> model1 -> post-hook1 -> pre-hook2 -> model2 -> post-hook2 Therefore, hook file requires more than one pre- and postprocessing functions. For doing that, preprocess and postprocess objects in the hooks file must be a list of functions with model-number length. None in the list may mean that there is no hook in this place.","title":"Launching multiple models"},{"location":"serving/kibernetika-serving/#multiple-models-hooks-file-example","text":"For the examples below, kibernetika-serving may be launched using the following command: kibernetika-serving --driver null --model-path any --driver null --model-path any2 Note: The example should work with null driver but you might be interested in changing --driver and --model-path params for using your own driver and model. The example merely shows a concept itself. import logging LOG = logging.getLogger(__name__) def log(func): def decorator(*args): LOG.info('Running %s...' % func.__name__) return func(*args) return decorator def init_hook(**params): LOG.info(\"Got params:\") LOG.info(params) @log def preprocess1(inputs): return inputs @log def preprocess2(inputs): return inputs @log def postprocess1(outputs): return outputs @log def postprocess2(outputs): return outputs preprocess = [preprocess1, preprocess2] postprocess = [postprocess1, postprocess2] Partially implemented hooks: import logging LOG = logging.getLogger(__name__) def log(func): def decorator(*args): LOG.info('Running %s...' % func.__name__) return func(*args) return decorator def init_hook(**params): LOG.info(\"Got params:\") LOG.info(params) @log def preprocess1(inputs): return inputs @log def postprocess2(outputs): return outputs preprocess = [preprocess1, None] postprocess = [None, postprocess2]","title":"Multiple models hooks file example"},{"location":"serving/kibernetika-serving/#ignore-skip-model-inference","text":"In some cases it needs to skip model inference (more useful when using multiple models). For example, first model detects faces and the second one - emotions on this faces. There is a case where there are no faces on image at all - therefore emotion detection doesn't make sense anymore because it requires face boxes. Example : def preprocess1(inputs): return inputs def postprocess1(outputs, ctx): # if length of face boxes is 0 -> skip_next is True else False ctx.skip_next = len(outputs.get('face-boxes', [])) == 0 returnt outputs def preprocess2(inputs, ctx): if ctx.skip_next: inputs['ml-serving-ignore'] = True return inputs def postprocess2(outputs, ctx): # Was skipped? if ctx.skip_next: # Nothing to return in case if skipped return {} return outputs preprocess = [preprocess1, preprocess2] postprocess = [postprocess1, postprocess2]","title":"Ignore (skip) model inference"},{"location":"serving/serving-start/","text":"Model Serving # Once you have created a model and pushed it to the catalog, you will see the new model version on the Versions tab on the model view: Now you are ready to serve the model. Serving the model means that there will be a running TCP/HTTP server which accepts incoming requests with data in the appropriate format, runs inference with this data and give the output as the response. To serve the model, click the Serve button near appropriate version. You will see the serving configuration form which basically specifies execution command, environment, data and resources for the serving object. Let's see all the parameters in details. General parameters # Workspace - name of workspace, where serving will be run. Cluster - choose cluster where serving will be run physically. Name - specify a name for the serving. Execution command - Bash execution command. Supports any bash-like constructions. Usually, execution command for serving is a call to model runner command line tool, such as: tensorflow_model_server kibernetika-serving For tensorflow_model_server , it may be like tensorflow_model_server --model_base_path=<model_versions_dir> --model_name=my-model --port=9000 For kibernetika-serving , it may be like kibernetika-serving --driver tensorflow --model-path <saved-model-dir> --port 9000 Note: For the kibernetika-serving tool description and details, see here . Work Directory - specifies command current directory. Usually set to one of volume directory alias - such as $SRC_DIR Resources # CPU - CPU resource request (examples: 500m (0.5Core), 2 (2 Cores)) CPU limit - CPU resource limit Memory - Memory resource request (examples: 512M (512 MB), 2Gi (2 GB)) Memory limit - Memory resource limit GPU - Number of allocated GPUs per each replica Replicas - Number of replicas (basically, the number of servers) Images # CPU - Docker Image used when not using GPU. GPU - Docker Image used for GPU Note : For kibernetika-serving , image kuberlab/serving:latest is usually used, see kibernetika-serving document for the details. Environment variables - pass additional environment variables Ports - specify ports which will be opened in container: name , protocol , port number Volumes - TBD (See detailed description at the Volumes page) Serving parameters # Serving parameters only needed for the correct filling and passing parameters values from the Web UI: that is, you can pass number or string to your serving, or even upload a file (say, a picture). Output filter - only specified parameter names will be returned to the UI, e.g. output - will return only output key-value in JSON. Model - UI will pass the given model name to the serving (important to tensorflow_model_server -based servings). Signature - UI will pass the given model graph signature to the serving (applied only tensorflow_model_server -based servings). Output MIME Type - is specified, the UI will treat the output as this MIME type. Raw input - If marked, inputs sent in data key inputs instead of older features . Params # Specify a set of input parameters: their name and type. Types can be the following: int8 int16 int32 int64 int (basically it is int32) float double byte - if this specified, the form for file uploading is shown in the UI. Data is passed to the serving as byte array. bytes - if this specified, the form for file uploading is shown in the UI. Data is passed to the serving as an array of byte arrays containing 1 element. string uint8 uint16 On the started serving, you can pass multiple numbers (for numeric types) separating them by commas.","title":"Start serving"},{"location":"serving/serving-start/#model-serving","text":"Once you have created a model and pushed it to the catalog, you will see the new model version on the Versions tab on the model view: Now you are ready to serve the model. Serving the model means that there will be a running TCP/HTTP server which accepts incoming requests with data in the appropriate format, runs inference with this data and give the output as the response. To serve the model, click the Serve button near appropriate version. You will see the serving configuration form which basically specifies execution command, environment, data and resources for the serving object. Let's see all the parameters in details.","title":"Model Serving"},{"location":"serving/serving-start/#general-parameters","text":"Workspace - name of workspace, where serving will be run. Cluster - choose cluster where serving will be run physically. Name - specify a name for the serving. Execution command - Bash execution command. Supports any bash-like constructions. Usually, execution command for serving is a call to model runner command line tool, such as: tensorflow_model_server kibernetika-serving For tensorflow_model_server , it may be like tensorflow_model_server --model_base_path=<model_versions_dir> --model_name=my-model --port=9000 For kibernetika-serving , it may be like kibernetika-serving --driver tensorflow --model-path <saved-model-dir> --port 9000 Note: For the kibernetika-serving tool description and details, see here . Work Directory - specifies command current directory. Usually set to one of volume directory alias - such as $SRC_DIR","title":"General parameters"},{"location":"serving/serving-start/#resources","text":"CPU - CPU resource request (examples: 500m (0.5Core), 2 (2 Cores)) CPU limit - CPU resource limit Memory - Memory resource request (examples: 512M (512 MB), 2Gi (2 GB)) Memory limit - Memory resource limit GPU - Number of allocated GPUs per each replica Replicas - Number of replicas (basically, the number of servers)","title":"Resources"},{"location":"serving/serving-start/#images","text":"CPU - Docker Image used when not using GPU. GPU - Docker Image used for GPU Note : For kibernetika-serving , image kuberlab/serving:latest is usually used, see kibernetika-serving document for the details. Environment variables - pass additional environment variables Ports - specify ports which will be opened in container: name , protocol , port number Volumes - TBD (See detailed description at the Volumes page)","title":"Images"},{"location":"serving/serving-start/#serving-parameters","text":"Serving parameters only needed for the correct filling and passing parameters values from the Web UI: that is, you can pass number or string to your serving, or even upload a file (say, a picture). Output filter - only specified parameter names will be returned to the UI, e.g. output - will return only output key-value in JSON. Model - UI will pass the given model name to the serving (important to tensorflow_model_server -based servings). Signature - UI will pass the given model graph signature to the serving (applied only tensorflow_model_server -based servings). Output MIME Type - is specified, the UI will treat the output as this MIME type. Raw input - If marked, inputs sent in data key inputs instead of older features .","title":"Serving parameters"},{"location":"serving/serving-start/#params","text":"Specify a set of input parameters: their name and type. Types can be the following: int8 int16 int32 int64 int (basically it is int32) float double byte - if this specified, the form for file uploading is shown in the UI. Data is passed to the serving as byte array. bytes - if this specified, the form for file uploading is shown in the UI. Data is passed to the serving as an array of byte arrays containing 1 element. string uint8 uint16 On the started serving, you can pass multiple numbers (for numeric types) separating them by commas.","title":"Params"},{"location":"settings/billing/","text":"Billing # In Kibernetika Machine Teaching Platform deployed on public cloud Billing tab will be available in the settings.","title":"Billing"},{"location":"settings/billing/#billing","text":"In Kibernetika Machine Teaching Platform deployed on public cloud Billing tab will be available in the settings.","title":"Billing"},{"location":"settings/organization/","text":"Organizations # An organization is a construct used to centralize projects and resources that are inter-related and need to be managed under a single umbrella. These projects and resources then live under the organization, and billing goes through a single central organization account. You can create different teams under the organization hierarchy, set up access permissions for each team and invite specific users to each team. Creating a new organization # Creating a new organization is done from the 'settings' page. You will need to fill in the following fields: Title - The visible name of the organization. Identification - This is filled automatically with a default. You can provide your own identification by changing the default. This field is used in the URL which points to your organization. A valid identification string must be 63 characters or less and must begin and end with an lower case alphanumeric character ([a-z0-9]). Between these, it may have dashes (-) and lower case alphanumeric characters. Phone - Contact phone number for the organization. URL - Web page for the organization. Creating Teams # After an organization has been created, you can create different teams under the organization and set up access groups for those teams. By default, there is only one team named 'Owners'. The users in this team have full permissions that allow them to manage Infrastructures, Projects, Teams, Billing Information(if available) and Users for that organization. Service Accounts # See user service accounts for details. Billing # See user billing for details.","title":"Organization"},{"location":"settings/organization/#organizations","text":"An organization is a construct used to centralize projects and resources that are inter-related and need to be managed under a single umbrella. These projects and resources then live under the organization, and billing goes through a single central organization account. You can create different teams under the organization hierarchy, set up access permissions for each team and invite specific users to each team.","title":"Organizations"},{"location":"settings/organization/#creating-a-new-organization","text":"Creating a new organization is done from the 'settings' page. You will need to fill in the following fields: Title - The visible name of the organization. Identification - This is filled automatically with a default. You can provide your own identification by changing the default. This field is used in the URL which points to your organization. A valid identification string must be 63 characters or less and must begin and end with an lower case alphanumeric character ([a-z0-9]). Between these, it may have dashes (-) and lower case alphanumeric characters. Phone - Contact phone number for the organization. URL - Web page for the organization.","title":"Creating a new organization"},{"location":"settings/organization/#creating-teams","text":"After an organization has been created, you can create different teams under the organization and set up access groups for those teams. By default, there is only one team named 'Owners'. The users in this team have full permissions that allow them to manage Infrastructures, Projects, Teams, Billing Information(if available) and Users for that organization.","title":"Creating Teams"},{"location":"settings/organization/#service-accounts","text":"See user service accounts for details.","title":"Service Accounts"},{"location":"settings/organization/#billing","text":"See user billing for details.","title":"Billing"},{"location":"settings/user/","text":"User Settings # To change personal info or configure access to external repositories and service account go to settings page. Repositories # You can add OAuth based connection to your external git repositories or to your google account. This is required if you are planning to create your own clusters or want to be able to connect your git repositories to your projects as a storage volume. Following OAuth connections is supported: Github BitBucket Google Cloud Service Accounts # Used for manage private information such as secrets,keys or password for external resources. You could add following Service accounts: Google - OAuth credentials. Required for creating and provisioning your own Clusters on the Google cloud. Amazon - AWS access ID and secret key. Required for creating and provisioning your own Clusters on the AWS or to connecting AWS S3 Bucket as data source to your project. GIT - Private deploy key or user name and access token. Required for connecting private git repositories as data source to your project. See Git data source for details and using deploy keys or using access token User Token # You can create a personal access token and use it in place of a password for Kibernetika API or for CLI tools. Note: token is displaying only once just after creating. Copy it to safe place right after creation, it will be unable to restore token, it needs to create new token if old one has been lost. Delete User # Delete User will allow you to completely remove your user from Kibernetika service. This action will delete your account and all data associated with it. ATTENTION: Be careful, all your data will be lost after this action!","title":"User"},{"location":"settings/user/#user-settings","text":"To change personal info or configure access to external repositories and service account go to settings page.","title":"User Settings"},{"location":"settings/user/#repositories","text":"You can add OAuth based connection to your external git repositories or to your google account. This is required if you are planning to create your own clusters or want to be able to connect your git repositories to your projects as a storage volume. Following OAuth connections is supported: Github BitBucket Google Cloud","title":"Repositories"},{"location":"settings/user/#service-accounts","text":"Used for manage private information such as secrets,keys or password for external resources. You could add following Service accounts: Google - OAuth credentials. Required for creating and provisioning your own Clusters on the Google cloud. Amazon - AWS access ID and secret key. Required for creating and provisioning your own Clusters on the AWS or to connecting AWS S3 Bucket as data source to your project. GIT - Private deploy key or user name and access token. Required for connecting private git repositories as data source to your project. See Git data source for details and using deploy keys or using access token","title":"Service Accounts"},{"location":"settings/user/#user-token","text":"You can create a personal access token and use it in place of a password for Kibernetika API or for CLI tools. Note: token is displaying only once just after creating. Copy it to safe place right after creation, it will be unable to restore token, it needs to create new token if old one has been lost.","title":"User Token"},{"location":"settings/user/#delete-user","text":"Delete User will allow you to completely remove your user from Kibernetika service. This action will delete your account and all data associated with it. ATTENTION: Be careful, all your data will be lost after this action!","title":"Delete User"},{"location":"tools/kdataset/","text":"kdataset # Utility script for dataset management. Usage: kdataset <command> Available Commands: bash-completion : Generates bash completion scripts delete : Delete specific catalog entity. list : List catalog entities for the given workspace. help : Help for any kdataset command. pull : Download the data entity archive. push : Push the data within the current directory. version-delete : Delete specific version of the catalog entity. version-list : List versions for the given catalog entity. Flags: --config string Path to config file. (default ~/.kuberlab/config) --debug Enable debug level (shortcut for --log-level=debug). -h , --help help for kdataset --insecure Enable insecure SSL/TLS connection (skip verify). --log-level string Logging level. One of (debug, info, warning, error) (default \"info\") --secret string Kibernetika AI workspace secret (auth method) -t , --token string Kibernetika AI user token --type entityType Choose entityType type for request: [dataset model] (default dataset) --url string Base url to dataset storage. --version version for kdataset --workspace string Kibernetika AI workspace name (auth method) kdataset commands # delete # Usage: kdataset delete <workspace> <entity-name> Example: kdataset delete my-workspace dataset-to-delete list # Usage: kdataset list <workspace> Example(listing models): kdataset list my-workspace --type model pull # Usage: kdataset pull <workspace> <entity-name>:<version> [-O output-file.tar] Flags: -O , --output string : Output filename. Example: kdataset pull my-workspace big-dataset:1.0.1 Note : The command above will create a file named my-workspace-big-dataset.1.0.1.tar push # Usage: kdataset push <workspace> <entity-name>:<version> [flags] Flags: --chunk-size int: Chunk-size for scanning (default 1024000 ~ 1MB). -c , --concurrency int: Number of concurrent request to server (defaults to CPU cores num). --create : Create entity in catalog if not exists. --comment string: Comment for the new version -f , --force : Force uploading regardless warnings. --publish Newly created dataset will be public. Only used in conjunction with --create. Example (creating and publishing some dataset): kdataset push my-workspace big-dataset:1.0.1 --create --publish version-delete # Usage: kdataset version-delete <workspace> <entity-name>:<version> Example (deleting model version): kdataset --type model version-delete my-workspace tf-model:1.0.1 version-list # Usage: kdataset version-list <workspace> <entity-name> Example: kdataset version-list my-workspace big-dataset More Examples: # kdataset push test-projects cifar-10:1.0.0 kdataset push test-projects cifar-10:1.0.0 --create kdataset version-list test-projects cifar-10 kdataset pull test-projects cifar-10:1.0.0 Installation: # Download the version for your OS from the kdataset release page Uncompress the downloaded tarball. Copy the kdataset utility to the folder pointed to by \u201cPATH\u201d environment\u201d variable sudo cp kdataset /usr/local/bin To connect from the kdataset utility to the Kibernetika application, you need a Kibernetika config file at ~/.kuberlab/config . If you do not have one, you need to create one. The configuration values that need to be created are (basically, it is a simple file in YAML format): base_url: https://cloud.kibernetika.io/api/v0.2 # url to access Kibernetika API. token: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # your token, which can be obtained from the settings page of the Kibernetika application. Please refer to kdataset README.md for more detailed configuration. To verify the installation, at first use kdataset --version , to verify that you are executing the right version of utility Then execute kdataset list kuberlab-demo And you should see the list of all the demo data sets.","title":"kdataset"},{"location":"tools/kdataset/#kdataset","text":"Utility script for dataset management. Usage: kdataset <command> Available Commands: bash-completion : Generates bash completion scripts delete : Delete specific catalog entity. list : List catalog entities for the given workspace. help : Help for any kdataset command. pull : Download the data entity archive. push : Push the data within the current directory. version-delete : Delete specific version of the catalog entity. version-list : List versions for the given catalog entity. Flags: --config string Path to config file. (default ~/.kuberlab/config) --debug Enable debug level (shortcut for --log-level=debug). -h , --help help for kdataset --insecure Enable insecure SSL/TLS connection (skip verify). --log-level string Logging level. One of (debug, info, warning, error) (default \"info\") --secret string Kibernetika AI workspace secret (auth method) -t , --token string Kibernetika AI user token --type entityType Choose entityType type for request: [dataset model] (default dataset) --url string Base url to dataset storage. --version version for kdataset --workspace string Kibernetika AI workspace name (auth method)","title":"kdataset"},{"location":"tools/kdataset/#kdataset-commands","text":"","title":"kdataset commands"},{"location":"tools/kdataset/#delete","text":"Usage: kdataset delete <workspace> <entity-name> Example: kdataset delete my-workspace dataset-to-delete","title":"delete"},{"location":"tools/kdataset/#list","text":"Usage: kdataset list <workspace> Example(listing models): kdataset list my-workspace --type model","title":"list"},{"location":"tools/kdataset/#pull","text":"Usage: kdataset pull <workspace> <entity-name>:<version> [-O output-file.tar] Flags: -O , --output string : Output filename. Example: kdataset pull my-workspace big-dataset:1.0.1 Note : The command above will create a file named my-workspace-big-dataset.1.0.1.tar","title":"pull"},{"location":"tools/kdataset/#push","text":"Usage: kdataset push <workspace> <entity-name>:<version> [flags] Flags: --chunk-size int: Chunk-size for scanning (default 1024000 ~ 1MB). -c , --concurrency int: Number of concurrent request to server (defaults to CPU cores num). --create : Create entity in catalog if not exists. --comment string: Comment for the new version -f , --force : Force uploading regardless warnings. --publish Newly created dataset will be public. Only used in conjunction with --create. Example (creating and publishing some dataset): kdataset push my-workspace big-dataset:1.0.1 --create --publish","title":"push"},{"location":"tools/kdataset/#version-delete","text":"Usage: kdataset version-delete <workspace> <entity-name>:<version> Example (deleting model version): kdataset --type model version-delete my-workspace tf-model:1.0.1","title":"version-delete"},{"location":"tools/kdataset/#version-list","text":"Usage: kdataset version-list <workspace> <entity-name> Example: kdataset version-list my-workspace big-dataset","title":"version-list"},{"location":"tools/kdataset/#more-examples","text":"kdataset push test-projects cifar-10:1.0.0 kdataset push test-projects cifar-10:1.0.0 --create kdataset version-list test-projects cifar-10 kdataset pull test-projects cifar-10:1.0.0","title":"More Examples:"},{"location":"tools/kdataset/#installation","text":"Download the version for your OS from the kdataset release page Uncompress the downloaded tarball. Copy the kdataset utility to the folder pointed to by \u201cPATH\u201d environment\u201d variable sudo cp kdataset /usr/local/bin To connect from the kdataset utility to the Kibernetika application, you need a Kibernetika config file at ~/.kuberlab/config . If you do not have one, you need to create one. The configuration values that need to be created are (basically, it is a simple file in YAML format): base_url: https://cloud.kibernetika.io/api/v0.2 # url to access Kibernetika API. token: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # your token, which can be obtained from the settings page of the Kibernetika application. Please refer to kdataset README.md for more detailed configuration. To verify the installation, at first use kdataset --version , to verify that you are executing the right version of utility Then execute kdataset list kuberlab-demo And you should see the list of all the demo data sets.","title":"Installation:"},{"location":"tools/python-mlboardclient/","text":"python-mlboardclient # python-mlboardclient is a python library for interacting ml-board component. Basically it can manipulate with current project, it's tasks and servings. Also it provides the API for datasets and models. Contents # Getting started Uploading the model Working with tasks Dive into tasks TensorFlow config for distributed training Datasets and models","title":"Index"},{"location":"tools/python-mlboardclient/#python-mlboardclient","text":"python-mlboardclient is a python library for interacting ml-board component. Basically it can manipulate with current project, it's tasks and servings. Also it provides the API for datasets and models.","title":"python-mlboardclient"},{"location":"tools/python-mlboardclient/#contents","text":"Getting started Uploading the model Working with tasks Dive into tasks TensorFlow config for distributed training Datasets and models","title":"Contents"},{"location":"tools/python-mlboardclient/datasets-and-models/","text":"Datasets and models # This section describes data entities API (datasets and models). python-mlboardclient allows to manage datasets and models: in this case mlboardclient acts as a thin layer to another CLI - kdataset . Managing datasets and models # Once the mlboardclient is initialized, it can be used to leverage datasets (for examples below, instance of mlboardclient is in mlboard variable) Note : All methods below used almost identically for either a dataset or a model . To make call for the model , pass argument type='model' instead of type='dataset' which is used by default. List datasets (or models) # def list(self, workspace, type='dataset') Lists datasets for the given workspace. Note: If inside the project, it is possible to take workspace name from environment variable WORKSPACE_NAME . Examples: datasets = mlboard.datasets.list('my-workspace', type='dataset') print(datasets) import os datasets = mlboard.datasets.list(os.environ['WORKSPACE_NAME'], type='dataset') List version for specific dataset (or model)** # def version_list(self, workspace, name, type='dataset') Lists versions for the given catalog entity. Note: If inside the project, it is possible to take workspace name from environment variable WORKSPACE_NAME . Example: v_list = mlboard.datasets.version_list('my-workspace', 'my-dataset', type='dataset') print(v_list) Pull (download) dataset (or model) # def pull(self, workspace, name, version, to_dir, type='dataset', file_name=None): Downloads the data entity tar archive to the specified location. If file_name is None , then entity is downloaded to file named <workspace>-<dataset>.<version>.tar . If to_dir is empty, then current working directory is used. Example: mlboard.datasets.pull('my-workspace', 'my-dataset', '1.0.0', '', type='dataset') Push (upload) dataset (or model) # def push(self, workspace, name, version, from_dir, type='dataset', create=False, publish=False, force=False, chunk_size=None, concurrency=None, spec=None): Push the data within the specified directory. If file_name is None , then entity is downloaded to file named <workspace>-<dataset>.<version>.tar . If from_dir is empty, then current working directory is used. If create is True , then entity will be created if not exists. If publish is True , then entity will be public when created. If force is True , then entity will be created regardless some warnings. chunk_size is used to specify chunk size for every file in dataset (default 1024000 ) concurrency is used to specify number of concurrent connections (defaults to <cores_num * 2> ) spec used only if pushing a model . It is a dict of model spec for serving (or compatible json-string). The client automatically picks up a spec from ML project if any exists. See more details at Upload a model . Example: mlboard.datasets.push('my-workspace', 'my-dataset', '1.0.0', '/model/path', type='dataset') Delete dataset (or model) # delete(self, workspace, name, type='dataset') Deletes specific catalog entity. Example: mlboard.datasets.delete('my-workspace', 'my-dataset') Delete version of dataset (or model) # def version_delete(self, workspace, name, version, type='dataset') Delete specific version of the catalog entity. Example: mlboard.datasets.version_delete('my-workspace', 'my-dataset', '1.0.0')","title":"Datasets and Models"},{"location":"tools/python-mlboardclient/datasets-and-models/#datasets-and-models","text":"This section describes data entities API (datasets and models). python-mlboardclient allows to manage datasets and models: in this case mlboardclient acts as a thin layer to another CLI - kdataset .","title":"Datasets and models"},{"location":"tools/python-mlboardclient/datasets-and-models/#managing-datasets-and-models","text":"Once the mlboardclient is initialized, it can be used to leverage datasets (for examples below, instance of mlboardclient is in mlboard variable) Note : All methods below used almost identically for either a dataset or a model . To make call for the model , pass argument type='model' instead of type='dataset' which is used by default.","title":"Managing datasets and models"},{"location":"tools/python-mlboardclient/datasets-and-models/#list-datasets-or-models","text":"def list(self, workspace, type='dataset') Lists datasets for the given workspace. Note: If inside the project, it is possible to take workspace name from environment variable WORKSPACE_NAME . Examples: datasets = mlboard.datasets.list('my-workspace', type='dataset') print(datasets) import os datasets = mlboard.datasets.list(os.environ['WORKSPACE_NAME'], type='dataset')","title":"List datasets (or models)"},{"location":"tools/python-mlboardclient/datasets-and-models/#list-version-for-specific-dataset-or-model","text":"def version_list(self, workspace, name, type='dataset') Lists versions for the given catalog entity. Note: If inside the project, it is possible to take workspace name from environment variable WORKSPACE_NAME . Example: v_list = mlboard.datasets.version_list('my-workspace', 'my-dataset', type='dataset') print(v_list)","title":"List version for specific dataset (or model)**"},{"location":"tools/python-mlboardclient/datasets-and-models/#pull-download-dataset-or-model","text":"def pull(self, workspace, name, version, to_dir, type='dataset', file_name=None): Downloads the data entity tar archive to the specified location. If file_name is None , then entity is downloaded to file named <workspace>-<dataset>.<version>.tar . If to_dir is empty, then current working directory is used. Example: mlboard.datasets.pull('my-workspace', 'my-dataset', '1.0.0', '', type='dataset')","title":"Pull (download) dataset (or model)"},{"location":"tools/python-mlboardclient/datasets-and-models/#push-upload-dataset-or-model","text":"def push(self, workspace, name, version, from_dir, type='dataset', create=False, publish=False, force=False, chunk_size=None, concurrency=None, spec=None): Push the data within the specified directory. If file_name is None , then entity is downloaded to file named <workspace>-<dataset>.<version>.tar . If from_dir is empty, then current working directory is used. If create is True , then entity will be created if not exists. If publish is True , then entity will be public when created. If force is True , then entity will be created regardless some warnings. chunk_size is used to specify chunk size for every file in dataset (default 1024000 ) concurrency is used to specify number of concurrent connections (defaults to <cores_num * 2> ) spec used only if pushing a model . It is a dict of model spec for serving (or compatible json-string). The client automatically picks up a spec from ML project if any exists. See more details at Upload a model . Example: mlboard.datasets.push('my-workspace', 'my-dataset', '1.0.0', '/model/path', type='dataset')","title":"Push (upload) dataset (or model)"},{"location":"tools/python-mlboardclient/datasets-and-models/#delete-dataset-or-model","text":"delete(self, workspace, name, type='dataset') Deletes specific catalog entity. Example: mlboard.datasets.delete('my-workspace', 'my-dataset')","title":"Delete dataset (or model)"},{"location":"tools/python-mlboardclient/datasets-and-models/#delete-version-of-dataset-or-model","text":"def version_delete(self, workspace, name, version, type='dataset') Delete specific version of the catalog entity. Example: mlboard.datasets.version_delete('my-workspace', 'my-dataset', '1.0.0')","title":"Delete version of dataset (or model)"},{"location":"tools/python-mlboardclient/dive-into-tasks/","text":"Dive into tasks # This section describes nuances and details of working with tasks API in mlboardclient . Working with raw task config # Task config field contains task configuration including resources config. Task config can be rewritten before start using: task.apply_env(envs) # It adds/modifies environment variables for task resources. envs parameter must be a list containing env var specs as following: ENV_VAR=VALUE Example: task.apply_env(['MY_VAR=MY-VAL', 'SOME_VAR=expression=22']) In the example above variable 'SOME_VAR' will be assigned value 'expression=22'. task.apply_resource_overrides() # Overrides resource-specific config values for given task. Resource override spec format: <resource-name>.<param>=<value> or <resource-name>.<key>.<nested-key>=<value> It is allowed to pass '*' as resource name: override will be applied to all resources: *.<key>=<value> Examples: task.apply_resource_overrides([ '*.resources.requests.cpu=1000m', '*.resources.requests.memory=1Gi' ]) task.apply_resource_overrides(['worker.replicas=3']) Passing raw arguments dict # Also it is possible to pass additional command arguments as dict. For doing so, args key may be used. All arguments passed in args in form of dict, e.g. {\"var\": \"val\", \"arg2\", \"val2\"} will be appended to execution command as --var val --arg2 val2 : >>> task.config['resources'][0]['command'] 'python my_script.py' >>> task.config['resources'][0]['args'] = { ... 'var1': 'val', 'batch-size': 23, 'test_arg': 12.2 ... } >>> task.start() >>> task.config['resources'][0]['command'] 'python my_script.py --batch-size 23 --test_arg 12.2 --var1 val' If args is a string, then it is appended directly as a string, e.g: >>> task.config['resources'][0]['command'] 'python my_script.py' >>> task.config['resources'][0]['args'] = '--my-super-arg 42; echo \"Done!\"' >>> task.start() >>> task.config['resources'][0]['command'] 'python my_script.py --my-super-arg 42; echo \"Done!\"' Serving from task and checkpoint variable # When a task completes, it is possible to start a serving job from this task. For doing so, the form requires model path (a place where the model located) to run the serving. In order to automate passing model path to the serving job there is a couple of variables which may be exported in task (via update_task_info() and then further be used in serving form from task. Serving job accepts the following variables for passing the path: checkpoint_path model_path Example: Assuming the task generates a model at $TRAINING_DIR/model , the script will be: import os from os import path from mlboardclient.api import client mlboard = client.Client() data = { \"model_path\": path.join(os.environ['TRAINING_DIR'], 'model') } mlboard.update_task_info(data) After that, go to the Jobs tab in the UI, find this task, click ... and then Serve . Note : For picking up the variable, the execution command must refer the variable as usual in a bash script, i.e. $model_path Note 2 : For better experience, project config should have already prepared serving configuration and an execution command which uses needed variable, e.g. TensorFlow standard template uses $checkpoint_path : gitlab.kuberlab.io/kuberlab/tensorflow","title":"Dive into tasks"},{"location":"tools/python-mlboardclient/dive-into-tasks/#dive-into-tasks","text":"This section describes nuances and details of working with tasks API in mlboardclient .","title":"Dive into tasks"},{"location":"tools/python-mlboardclient/dive-into-tasks/#working-with-raw-task-config","text":"Task config field contains task configuration including resources config. Task config can be rewritten before start using:","title":"Working with raw task config"},{"location":"tools/python-mlboardclient/dive-into-tasks/#taskapply_envenvs","text":"It adds/modifies environment variables for task resources. envs parameter must be a list containing env var specs as following: ENV_VAR=VALUE Example: task.apply_env(['MY_VAR=MY-VAL', 'SOME_VAR=expression=22']) In the example above variable 'SOME_VAR' will be assigned value 'expression=22'.","title":"task.apply_env(envs)"},{"location":"tools/python-mlboardclient/dive-into-tasks/#taskapply_resource_overrides","text":"Overrides resource-specific config values for given task. Resource override spec format: <resource-name>.<param>=<value> or <resource-name>.<key>.<nested-key>=<value> It is allowed to pass '*' as resource name: override will be applied to all resources: *.<key>=<value> Examples: task.apply_resource_overrides([ '*.resources.requests.cpu=1000m', '*.resources.requests.memory=1Gi' ]) task.apply_resource_overrides(['worker.replicas=3'])","title":"task.apply_resource_overrides()"},{"location":"tools/python-mlboardclient/dive-into-tasks/#passing-raw-arguments-dict","text":"Also it is possible to pass additional command arguments as dict. For doing so, args key may be used. All arguments passed in args in form of dict, e.g. {\"var\": \"val\", \"arg2\", \"val2\"} will be appended to execution command as --var val --arg2 val2 : >>> task.config['resources'][0]['command'] 'python my_script.py' >>> task.config['resources'][0]['args'] = { ... 'var1': 'val', 'batch-size': 23, 'test_arg': 12.2 ... } >>> task.start() >>> task.config['resources'][0]['command'] 'python my_script.py --batch-size 23 --test_arg 12.2 --var1 val' If args is a string, then it is appended directly as a string, e.g: >>> task.config['resources'][0]['command'] 'python my_script.py' >>> task.config['resources'][0]['args'] = '--my-super-arg 42; echo \"Done!\"' >>> task.start() >>> task.config['resources'][0]['command'] 'python my_script.py --my-super-arg 42; echo \"Done!\"'","title":"Passing raw arguments dict"},{"location":"tools/python-mlboardclient/dive-into-tasks/#serving-from-task-and-checkpoint-variable","text":"When a task completes, it is possible to start a serving job from this task. For doing so, the form requires model path (a place where the model located) to run the serving. In order to automate passing model path to the serving job there is a couple of variables which may be exported in task (via update_task_info() and then further be used in serving form from task. Serving job accepts the following variables for passing the path: checkpoint_path model_path Example: Assuming the task generates a model at $TRAINING_DIR/model , the script will be: import os from os import path from mlboardclient.api import client mlboard = client.Client() data = { \"model_path\": path.join(os.environ['TRAINING_DIR'], 'model') } mlboard.update_task_info(data) After that, go to the Jobs tab in the UI, find this task, click ... and then Serve . Note : For picking up the variable, the execution command must refer the variable as usual in a bash script, i.e. $model_path Note 2 : For better experience, project config should have already prepared serving configuration and an execution command which uses needed variable, e.g. TensorFlow standard template uses $checkpoint_path : gitlab.kuberlab.io/kuberlab/tensorflow","title":"Serving from task and checkpoint variable"},{"location":"tools/python-mlboardclient/getting-started/","text":"python-mlboardclient # Python library for interacting ml-board component. Basically it can manipulate with current project, it's tasks and servings. Also it provides the API for datasets and models. Installation # From PyPi # pip install python-mlboardclient From github (fresh master branch) # pip install 'git+https://github.com/kuberlab/python-mlboardclient.git' Usage # from mlboardclient.api import client # Default url is http://mlboard-v2.kuberlab:8082/api/v2 # No need to pass any url if instantiate client inside ml-project (Jupyter/Task) ml = client.Client() # Get current project object app = ml.apps.get() # <App name=21-facenet-openvino55 revision=aba73d27564106a45d9439856c1856d562d9219f> # Get tasks from config app.tasks #[<Task name=align-images build=None status=undefined>, # <Task name=train-classifier build=None status=undefined>, # <Task name=validate-classifier build=None status=undefined>, # <Task name=pipeline build=None status=undefined>, # <Task name=model-converter build=None status=undefined>] task = app.tasks[0] # Start & wait task task = task.start() # <Task name=align-images build=4 status=Starting> # ... wait some time and refresh task task.refresh() # <Task name=align-images build=4 status=Running> # ... or wait task until it completed: task.wait() # <Task name=align-images build=4 status=Succeeded> # Get tasks from API app.get_tasks() #[<Task name=validate-classifier build=1 status=Failed>, # <Task name=train-classifier build=2 status=Failed>, # <Task name=model-converted build=3 status=Failed>, # <Task name=align-images build=4 status=Succeeded>] Model upload # ml.model_upload('my-model', '1.0.0', '/model/dir') # If the model uploading is not executing in scope of project task, # need to specify workspace explicitly: ml.model_upload( 'my-model', '1.0.0', '/model/dir', workspace='demo', ) # Wait until model is being uploaded.","title":"Getting started"},{"location":"tools/python-mlboardclient/getting-started/#python-mlboardclient","text":"Python library for interacting ml-board component. Basically it can manipulate with current project, it's tasks and servings. Also it provides the API for datasets and models.","title":"python-mlboardclient"},{"location":"tools/python-mlboardclient/getting-started/#installation","text":"","title":"Installation"},{"location":"tools/python-mlboardclient/getting-started/#from-pypi","text":"pip install python-mlboardclient","title":"From PyPi"},{"location":"tools/python-mlboardclient/getting-started/#from-github-fresh-master-branch","text":"pip install 'git+https://github.com/kuberlab/python-mlboardclient.git'","title":"From github (fresh master branch)"},{"location":"tools/python-mlboardclient/getting-started/#usage","text":"from mlboardclient.api import client # Default url is http://mlboard-v2.kuberlab:8082/api/v2 # No need to pass any url if instantiate client inside ml-project (Jupyter/Task) ml = client.Client() # Get current project object app = ml.apps.get() # <App name=21-facenet-openvino55 revision=aba73d27564106a45d9439856c1856d562d9219f> # Get tasks from config app.tasks #[<Task name=align-images build=None status=undefined>, # <Task name=train-classifier build=None status=undefined>, # <Task name=validate-classifier build=None status=undefined>, # <Task name=pipeline build=None status=undefined>, # <Task name=model-converter build=None status=undefined>] task = app.tasks[0] # Start & wait task task = task.start() # <Task name=align-images build=4 status=Starting> # ... wait some time and refresh task task.refresh() # <Task name=align-images build=4 status=Running> # ... or wait task until it completed: task.wait() # <Task name=align-images build=4 status=Succeeded> # Get tasks from API app.get_tasks() #[<Task name=validate-classifier build=1 status=Failed>, # <Task name=train-classifier build=2 status=Failed>, # <Task name=model-converted build=3 status=Failed>, # <Task name=align-images build=4 status=Succeeded>]","title":"Usage"},{"location":"tools/python-mlboardclient/getting-started/#model-upload","text":"ml.model_upload('my-model', '1.0.0', '/model/dir') # If the model uploading is not executing in scope of project task, # need to specify workspace explicitly: ml.model_upload( 'my-model', '1.0.0', '/model/dir', workspace='demo', ) # Wait until model is being uploaded.","title":"Model upload"},{"location":"tools/python-mlboardclient/tf-conf-gen/","text":"python-mlboardclient contains the tf_conf script which can be used to generate TensorFlow config for distributed training: For worker: TF_CONFIG=$(tf_conf worker) python <train-script.py> For parameter server: TF_CONFIG=$(tf_conf ps) python <train-script.py>","title":"TensorFlow distribute configuration"},{"location":"tools/python-mlboardclient/upload-model/","text":"Uploading the model # Using mlboardclient it is possible to upload the catalog as a model. Once the client have been initialized, it may be used as the following: mlboard.model_upload(\"model-name\", \"1.0.0-version\", \"/model-catalog-dir\") Full model_upload method spec: model_upload(self, model_name, version, path, workspace=None, auto_create=True, spec=None) Arguments: auto_create : if True, then model will be created if there is no such model yet. Defaults to True . spec : Optional model spec dict, it contains model specification used for starting the model as a serving. If None and this method is called inside ML project, then mlboardclient tries to get spec automatically from the project if it exists. See about model spec format below. workspace : Optinal parameter. Must be provided if the method is called outside of ML project. Model spec format # Model spec should be dict or corresponding JSON-string. Example is below: { \"displayName\": \"Serving\", \"name\": \"tensorflow-serving\", \"ports\": [ { \"protocol\": \"TCP\", \"targetPort\": 9000, \"name\": \"grpc\", \"port\": 9000 } ], \"command\": \"kuberlab-serving --port=9000 --model-path=$SRC_DIR\", \"sources\": [ { \"mountPath\": \"/src\", \"name\": \"src\" \"gitRepo\": { \"repository\": \"https://github.com/kuberlab-catalog/tensorflow\" } } ], \"images\": { \"gpu\": \"kuberlab/serving:latest-gpu\", \"cpu\": \"kuberlab/serving:latest\" }, \"spec\": { \"outFilter\": \"string\", \"rawInput\": true, \"model\": \"string\", \"params\": [ { \"name\": \"input\", \"type\": \"bytes\" } ] }, \"resources\": { \"accelerators\": { \"gpu\": 0 }, \"requests\": { \"cpu\": \"100m\", \"memory\": \"125Mi\" }, \"limits\": { \"cpu\": \"1\", \"memory\": \"4Gi\" } } } Note : spec section is needed mostly for the web UI, to pass the data to the serving via web UI.","title":"Uploading a model"},{"location":"tools/python-mlboardclient/upload-model/#uploading-the-model","text":"Using mlboardclient it is possible to upload the catalog as a model. Once the client have been initialized, it may be used as the following: mlboard.model_upload(\"model-name\", \"1.0.0-version\", \"/model-catalog-dir\") Full model_upload method spec: model_upload(self, model_name, version, path, workspace=None, auto_create=True, spec=None) Arguments: auto_create : if True, then model will be created if there is no such model yet. Defaults to True . spec : Optional model spec dict, it contains model specification used for starting the model as a serving. If None and this method is called inside ML project, then mlboardclient tries to get spec automatically from the project if it exists. See about model spec format below. workspace : Optinal parameter. Must be provided if the method is called outside of ML project.","title":"Uploading the model"},{"location":"tools/python-mlboardclient/upload-model/#model-spec-format","text":"Model spec should be dict or corresponding JSON-string. Example is below: { \"displayName\": \"Serving\", \"name\": \"tensorflow-serving\", \"ports\": [ { \"protocol\": \"TCP\", \"targetPort\": 9000, \"name\": \"grpc\", \"port\": 9000 } ], \"command\": \"kuberlab-serving --port=9000 --model-path=$SRC_DIR\", \"sources\": [ { \"mountPath\": \"/src\", \"name\": \"src\" \"gitRepo\": { \"repository\": \"https://github.com/kuberlab-catalog/tensorflow\" } } ], \"images\": { \"gpu\": \"kuberlab/serving:latest-gpu\", \"cpu\": \"kuberlab/serving:latest\" }, \"spec\": { \"outFilter\": \"string\", \"rawInput\": true, \"model\": \"string\", \"params\": [ { \"name\": \"input\", \"type\": \"bytes\" } ] }, \"resources\": { \"accelerators\": { \"gpu\": 0 }, \"requests\": { \"cpu\": \"100m\", \"memory\": \"125Mi\" }, \"limits\": { \"cpu\": \"1\", \"memory\": \"4Gi\" } } } Note : spec section is needed mostly for the web UI, to pass the data to the serving via web UI.","title":"Model spec format"},{"location":"tools/python-mlboardclient/work-with-tasks/","text":"Working with tasks # This section describes the essential use cases in mlboardclient tasks API. Start, run and wait # To start another task using mlboardclient, need to get appropriate task config first: from mlboardclient.api import client mlboard = client.CLient() app = mlboard.apps.get() # Get task config from project config task = app.tasks.get('my-task') Once we get the task config, we can run it. This method starts a task, then automatically refresh it's state every 2 seconds and waiting task for complete. Once task is complete, it returns the completed task object: task.run() <Task name=my-task build=4 status=Succeeded> Another way to run task - is to control every stage manually. Start task first - method start() creates a new runtime task execution and immediately updates task properties: task.start() <Task name=my-task build=4 status=Starting> Task status can be updated using refresh() : task.refresh() <Task name=my-task build=4 status=Running> Possible task states : Starting , Pending , Running , Failed , Succeeded . The last 2 ones are the terminal states. Once the task is completed, it must have the status Failed or Succeeded. To check task for completeness, it is possible to see completed property: task = task.refresh() if task.completed: ... Then we can check for concrete task status: if task.completed: if task.status == 'Succeeded': print('Ok, my-task successful!') elif task.status == 'Failed: print('Ooops, task has been failed') # Or fail entire script Task logs # One more logical thing to do - is to grab task logs after or during execution. For instance, if a task failed, then it would be great to show it's logs to see what was happened: if task.status == 'Failed': print('Ooops, task has been failed, getting logs...') # Get the logs dict(pod_name => log) logs = task.logs() for k, v in logs.items(): print('Logs %s:\\n' % k) print(v) Code above will give all logs including the \"master\" pod (it is the most unnecessary one). Let's skip it: if task.status == 'Failed': print('Ooops, task has been failed, getting logs...') # Get the logs dict(pod_name => log) logs = task.logs() # Delete master log, we are interested only on workload log del logs['master'] for k, v in logs.items(): print('Logs %s:\\n' % k) print(v) Update task runtime properties # During task execution, it is important to know the info about critical and sagnificant values and variables. They can be exported using update_task_info() method and then they are shown in the UI: properties = { 'accuracy': 1, 'model_path': '/notebooks/model/20180402-114759.pb', 'mode': 'CLASSIFY', 'batch_size': 90, 'image_size': 160, 'model_uploaded': true, 'data_dir': '/notebooks/faces_160', 'num_classes': 8, 'num_images': 52, } task.update_task_info(properties)","title":"Working with tasks"},{"location":"tools/python-mlboardclient/work-with-tasks/#working-with-tasks","text":"This section describes the essential use cases in mlboardclient tasks API.","title":"Working with tasks"},{"location":"tools/python-mlboardclient/work-with-tasks/#start-run-and-wait","text":"To start another task using mlboardclient, need to get appropriate task config first: from mlboardclient.api import client mlboard = client.CLient() app = mlboard.apps.get() # Get task config from project config task = app.tasks.get('my-task') Once we get the task config, we can run it. This method starts a task, then automatically refresh it's state every 2 seconds and waiting task for complete. Once task is complete, it returns the completed task object: task.run() <Task name=my-task build=4 status=Succeeded> Another way to run task - is to control every stage manually. Start task first - method start() creates a new runtime task execution and immediately updates task properties: task.start() <Task name=my-task build=4 status=Starting> Task status can be updated using refresh() : task.refresh() <Task name=my-task build=4 status=Running> Possible task states : Starting , Pending , Running , Failed , Succeeded . The last 2 ones are the terminal states. Once the task is completed, it must have the status Failed or Succeeded. To check task for completeness, it is possible to see completed property: task = task.refresh() if task.completed: ... Then we can check for concrete task status: if task.completed: if task.status == 'Succeeded': print('Ok, my-task successful!') elif task.status == 'Failed: print('Ooops, task has been failed') # Or fail entire script","title":"Start, run and wait"},{"location":"tools/python-mlboardclient/work-with-tasks/#task-logs","text":"One more logical thing to do - is to grab task logs after or during execution. For instance, if a task failed, then it would be great to show it's logs to see what was happened: if task.status == 'Failed': print('Ooops, task has been failed, getting logs...') # Get the logs dict(pod_name => log) logs = task.logs() for k, v in logs.items(): print('Logs %s:\\n' % k) print(v) Code above will give all logs including the \"master\" pod (it is the most unnecessary one). Let's skip it: if task.status == 'Failed': print('Ooops, task has been failed, getting logs...') # Get the logs dict(pod_name => log) logs = task.logs() # Delete master log, we are interested only on workload log del logs['master'] for k, v in logs.items(): print('Logs %s:\\n' % k) print(v)","title":"Task logs"},{"location":"tools/python-mlboardclient/work-with-tasks/#update-task-runtime-properties","text":"During task execution, it is important to know the info about critical and sagnificant values and variables. They can be exported using update_task_info() method and then they are shown in the UI: properties = { 'accuracy': 1, 'model_path': '/notebooks/model/20180402-114759.pb', 'mode': 'CLASSIFY', 'batch_size': 90, 'image_size': 160, 'model_uploaded': true, 'data_dir': '/notebooks/faces_160', 'num_classes': 8, 'num_images': 52, } task.update_task_info(properties)","title":"Update task runtime properties"},{"location":"tutorials/openvino-1/","text":"This tutorial is a walk through an end-to-end AI project creating a face detection and recognition application in Kibernetika . We will begin by selecting data sets creating a project and selecting models, setting up the infrastructure, training those models, and completing by re-training for future proofing. Before we begin, let\u2019s first address that there are two main steps to this project - face detection and face recognition. First, to detect that A face actually exists in the image and, second, to then recognize a specific face. Face detection includes Pnet, Rnet, and ONet neural nets to define face boundary boxes on a picture. Face recognition includes calculating face embeddings using Inception ResNet model and training SVM classifier. Prepare DataSet for training # Start by creating a new dataset. In Kibernetika platform, we can do this using Web UI, CLI client or provided API. We will use Web UI. Open our demo dataset https://cloud.kuberlab.io/openvino-demo/catalog/dataset/open-faces/versions/1.0.0 And FORK this dataset to one of your workspace Create Project # Once we have this data set, we can go ahead and create a project. Open pre defined project template https://cloud.kuberlab.io/kuberlab-demo/catalog/chart-mlapp-v2/openvino-facenet/readme/ And install it to your workspace ATTENTION: You MUST switch dataset during installation to your personal datasets that we created on the previous step!!! It is very important this project want work with datasets form other workspaces!!! The project is now available to use. Convert Tensorflow model to OpenVino format # NOTES: you can skip this step because our facenet model from catalog already has model in OpenVino format too. To Convert project to OpenVino format we are going to execute our first task model-converter Once model is converted the new model files now existing in the your personal model catalog. From here, we will switch over to use the generated openvino model. Open SOURCES of your project and switch facenet source to new created model NOTES: make sure that you choose new created model from your current workspace. Build face recognition model # This brings us to the next part of the project, when we can actually start to train the face recognition model. This includes 3 sub tasks that we cluster together into pipeline task - Align images, train SVM model, and validate results. NOTES: Use pipeline task to build model. This task will automatically start align,train and validate tasks. Otherwise you need adjust parameter of each task, default parameters suitable only for whole piplene. Align performs cropping and will create a new version of the dataset that contains only cropped images. The next task is to train SVM classifier using Inception ResNet and openvino inference engine. And the last task is validating the result and uploading the new model to model catalog. At this time, we generate a confusion matrix (confusion matrix keras) after the model validation to understand how accurate our new model is. Start Serving # After the model is built, we can start the inference portion of this project. Open JOBS tab of your project Choose last pipeline finished task and press start serv from context menu Leave all parameters as is Press start SERV Now you can simple test your model Start serving from model catalog # Also new facenet-classifier model will be pushed to Model catalog after our pipeline is done and validation was succeeded with accuracy more than 0.9. You can start serving directly from Model catalog. Choose new created facenet-classifier model in the your Model catalog From context menu chose edit and fill required serving parameters: You could press USE TEMPLATE button and choose OpenVino Serving to fill some common parameters for OpenVino Serving backend Execution command: kuberlab-serving --driver openvino --model-path $FACENET_DIR/facenet.xml --hooks serving_hook.py -o classifier=$MODEL_DIR/classifier.pkl -o flexible_batch_size=True -o resolutions=14x19,19x27,26x37,37x52,52x74,73x104,103x146,145x206,205x290 -o use_tf=true -o tf_path=$FACENET_DIR Execution directory: $SRC_DIR Resources: CPU Requests: 100m Memory Requests: 64Mi CPU Max: 4 Memory Max: 4Gi Replicas: 1 CPU image: kuberlab/serving:latest-openvino Ports: name: grps, protocol: TCP, port: 9000, target port: 9000 Following volume required: Source code for pre processing and post processing hook: Name: src SubPath: facenet/src Type: GIT Repository: https://github.com/kibernetika-ai/facenet Model for face detection and inception open-vino model: Name: facenet Type: Model Model: facenet-pretrained [kuberlab-demo] Or facenet model from your catalog that was created after model converting task Version: 2.0.0 or your version that should looks like 1.x.x-openvino-CPU-xxx Serving parameters. Required only for testing using Kibernetika UI. Out filter: output Model: any Out mime type: image/png Raw Input: Yes Params: name: input, type: bytes Choose version that you want to serve and press SERVE verify that all required parameter is filled as on previous step (sometimes it may require refresh page). NOTES: by default one additional volume model will be added automatically to your serving configuration that will be refer to current(selected) version of model. Don't remove it. Retrain Model # Since data is likely to change over time in the real world, we can easily extend out the original dataset and continuously retrain the model. We call this Continuous Production and is a core funcion of the Kibernetika platform. Simply set up a new version, 1.0.1, add another person, Alex, and some images for that person. Reopen the project, switch the dataset to the new version and rebuild the model. Again, once re-training and validation completed, you can see the model accuracy and then make informed decisions about deploying the new model to production. And, as a collaborative platform, you can easily share results, models, data, etc. with team members for a seamless rollout. For more information on how you can accelerate your AI project with Kibernetika , reach us at info@kibernetika.ai today!","title":"Using OpenVINO New"},{"location":"tutorials/openvino-1/#prepare-dataset-for-training","text":"Start by creating a new dataset. In Kibernetika platform, we can do this using Web UI, CLI client or provided API. We will use Web UI. Open our demo dataset https://cloud.kuberlab.io/openvino-demo/catalog/dataset/open-faces/versions/1.0.0 And FORK this dataset to one of your workspace","title":"Prepare DataSet for training"},{"location":"tutorials/openvino-1/#create-project","text":"Once we have this data set, we can go ahead and create a project. Open pre defined project template https://cloud.kuberlab.io/kuberlab-demo/catalog/chart-mlapp-v2/openvino-facenet/readme/ And install it to your workspace ATTENTION: You MUST switch dataset during installation to your personal datasets that we created on the previous step!!! It is very important this project want work with datasets form other workspaces!!! The project is now available to use.","title":"Create Project"},{"location":"tutorials/openvino-1/#convert-tensorflow-model-to-openvino-format","text":"NOTES: you can skip this step because our facenet model from catalog already has model in OpenVino format too. To Convert project to OpenVino format we are going to execute our first task model-converter Once model is converted the new model files now existing in the your personal model catalog. From here, we will switch over to use the generated openvino model. Open SOURCES of your project and switch facenet source to new created model NOTES: make sure that you choose new created model from your current workspace.","title":"Convert Tensorflow model to OpenVino format"},{"location":"tutorials/openvino-1/#build-face-recognition-model","text":"This brings us to the next part of the project, when we can actually start to train the face recognition model. This includes 3 sub tasks that we cluster together into pipeline task - Align images, train SVM model, and validate results. NOTES: Use pipeline task to build model. This task will automatically start align,train and validate tasks. Otherwise you need adjust parameter of each task, default parameters suitable only for whole piplene. Align performs cropping and will create a new version of the dataset that contains only cropped images. The next task is to train SVM classifier using Inception ResNet and openvino inference engine. And the last task is validating the result and uploading the new model to model catalog. At this time, we generate a confusion matrix (confusion matrix keras) after the model validation to understand how accurate our new model is.","title":"Build face recognition model"},{"location":"tutorials/openvino-1/#start-serving","text":"After the model is built, we can start the inference portion of this project. Open JOBS tab of your project Choose last pipeline finished task and press start serv from context menu Leave all parameters as is Press start SERV Now you can simple test your model","title":"Start Serving"},{"location":"tutorials/openvino-1/#start-serving-from-model-catalog","text":"Also new facenet-classifier model will be pushed to Model catalog after our pipeline is done and validation was succeeded with accuracy more than 0.9. You can start serving directly from Model catalog. Choose new created facenet-classifier model in the your Model catalog From context menu chose edit and fill required serving parameters: You could press USE TEMPLATE button and choose OpenVino Serving to fill some common parameters for OpenVino Serving backend Execution command: kuberlab-serving --driver openvino --model-path $FACENET_DIR/facenet.xml --hooks serving_hook.py -o classifier=$MODEL_DIR/classifier.pkl -o flexible_batch_size=True -o resolutions=14x19,19x27,26x37,37x52,52x74,73x104,103x146,145x206,205x290 -o use_tf=true -o tf_path=$FACENET_DIR Execution directory: $SRC_DIR Resources: CPU Requests: 100m Memory Requests: 64Mi CPU Max: 4 Memory Max: 4Gi Replicas: 1 CPU image: kuberlab/serving:latest-openvino Ports: name: grps, protocol: TCP, port: 9000, target port: 9000 Following volume required: Source code for pre processing and post processing hook: Name: src SubPath: facenet/src Type: GIT Repository: https://github.com/kibernetika-ai/facenet Model for face detection and inception open-vino model: Name: facenet Type: Model Model: facenet-pretrained [kuberlab-demo] Or facenet model from your catalog that was created after model converting task Version: 2.0.0 or your version that should looks like 1.x.x-openvino-CPU-xxx Serving parameters. Required only for testing using Kibernetika UI. Out filter: output Model: any Out mime type: image/png Raw Input: Yes Params: name: input, type: bytes Choose version that you want to serve and press SERVE verify that all required parameter is filled as on previous step (sometimes it may require refresh page). NOTES: by default one additional volume model will be added automatically to your serving configuration that will be refer to current(selected) version of model. Don't remove it.","title":"Start serving from model catalog"},{"location":"tutorials/openvino-1/#retrain-model","text":"Since data is likely to change over time in the real world, we can easily extend out the original dataset and continuously retrain the model. We call this Continuous Production and is a core funcion of the Kibernetika platform. Simply set up a new version, 1.0.1, add another person, Alex, and some images for that person. Reopen the project, switch the dataset to the new version and rebuild the model. Again, once re-training and validation completed, you can see the model accuracy and then make informed decisions about deploying the new model to production. And, as a collaborative platform, you can easily share results, models, data, etc. with team members for a seamless rollout. For more information on how you can accelerate your AI project with Kibernetika , reach us at info@kibernetika.ai today!","title":"Retrain Model"},{"location":"tutorials/openvino/","text":"This tutorial is a walk through an end-to-end AI project creating a face detection and recognition application in Kibernetika . We will begin by selecting data sets creating a project and selecting models, setting up the infrastructure, training those models, and completing by re-training for future proofing. Before we begin, let\u2019s first address that there are two main steps to this project - face detection and face recognition. First, to detect that A face actually exists in the image and, second, to then recognize a specific face. Face detection includes Pnet, Rnet, and ONet neural nets to define face boundary boxes on a picture. Face recognition includes calculating face embeddings using Inception ResNet model and training SVM classifier. Start by creating a new dataset. In Kibernetika platform, we can do this using Web UI, CLI client or provided API. We will use Web UI. Open catalog page in the workspace you are planning to work. It can be your personal workspace or organization workspace where you can invite other users to collaborate. In the catalog go to Datasets and create the new dataset. Let's name it open-faces. From the very beginning, we start by defining versions so we will be able to have better control throughout the life of this project. Select VERSIONS Tab and add the new version. You need to enter version in the x.x.x format. We then upload images of faces of the people we want to recognise, to the dataset. Creating folder for each person and adding the files to respective folders. It\u2019s required to have at least 5 pictures per person, also it will be great if those pictures are different poses, etc... We then create a new folder for pictures of another person, Nikolay, and upload those relevant files. For the last set of data, let's name the folder \"Other\" and upload some random pictures of different people to better train the system. The final step for prepping the data is to go ahead and commit it. Once we have this data set, we can go ahead and create a project. We will select the Kiberntika template for the pretrained tensorflow facenet model. You can view more info about this model or the many others included in the platform by viewing our github repository. Install the project by setting all required parameters and defining where the training data will reside by selecting the dataset which we previously created. Select dataset for your project. You can use newly created dataset, or existing dataset from the catalog. If you are working in your personal catalog you will need to FORK public dataset to you private catalog. The project is now available to use. However, because the project is in Tensorflow format, we need to convert all models to openvino format, we use openvino as the inference backend engine in that project. Next, we are going to execute our script to convert tensorflow models to openvino format. You can view this job in realtime to track progress. The model is converted with the new files now existing in the kibernetika catalog. From here, we will switch over to use the generated openvino model. Now, go to the SOURCES tab of the project, and edit the content of the \"model\" volume Change it to the model converted to OpenVINO This brings us to the next part of the project, when we can actually start to train the face recognition model. This includes 3 sub tasks that we cluster together into pipeline task - Align images, train SVM model, and validate results. Align performs cropping and will create a new version of the dataset that contains only cropped images. The next task is to train SVM classifier using Inception ResNet and openvino inference engine. And the last task is validating the result and uploading the new model to model catalog. At this time, we generate a confusion matrix (confusion matrix keras) after the model validation to understand how accurate our new model is. After the model is built, we can start the inference portion of this project. Open facenet-classifier model and the version which you just build from your project catalog. For \u201cServing\u201d, there are many parameters that we can define. We are setting OpenVino as a driver for the kibernetika.ai inference engine, filling required resources for inference, and docker images that will be used. Finally, we select a repository that contains additional business logic, for preprocessing and post processing model inputs and outputs. Now that everything is in place, we can start serving. Once completed, we can easily test the face detection and recognition service. Since data is likely to change over time in the real world, we can easily extend out the original dataset and continuously retrain the model. We call this Continuous Production and is a core funcion of the Kibernetika platform. Simply set up a new version, 1.0.1, add another person, Alex, and some images for that person. Reopen the project, switch the dataset to the new version and rebuild the model. Again, once re-training and validation completed, you can see the model accuracy and then make informed decisions about deploying the new model to production. And, as a collaborative platform, you can easily share results, models, data, etc. with team members for a seamless rollout. For more information on how you can accelerate your AI project with Kibernetika , reach us at info@kibernetika.ai today!","title":"Using OpenVINO"},{"location":"tutorials/project-page/","text":"Summary # The last string of the project path identifies the project. Switch view advanced / simply Select experement. Project menu. Disable - unload all project components. Reset - delete project metadata (start from scratch) Edit - configure project including UI components. Delete - complelely delete project and all it`s metadata. Download - download project configuration. All project componentsare loaded and there is two of them. Create descriptionof then project, README, notes. Edit Summary. Taks # List of Task to execute as stand-alone or part of workflow. Edit task list. Select SAVE or SAVE AND EXECUTE to save this task configuration or create and start a new Job Load task template. Volume revision. Configuration of the Task covers storage, environment variables, resources and command line execution environment for your scripts. Sources # Tab gives access to the storage. NFS, S3, GIT, KL Cluster Storage, Model and Dataset from KL Catalog exposed as a storage volume available to All components of then project. Add and configure volumes History # List of experiments and check-ins seved on the Tasks tab. Open dialog check-out version. By selectinga record in history list you can check-out the configuration for that record Jobs # Item from the list of executed Jobs. Job status. Log from worker task. Metrics # Selector to monitoring tasks, ui components or serving. Selector metrics. Telemetry Monitoring for specific Job and component executed. Lib # Manager package selector. User can install additional packages, not pre-configured into evironment template. Multiple packages can be specified as a comma separated list. Chackbox allows to see the complete list of packages installed in the environment. Status # Tab displays the status of components of the project. Jupyter, Tensorbord, Serving and Worker components for batch operations. If for some reason KL was not able to load specific component, the diagnostic messages will be provided. Jupyter # Jupyter Notebook UI interface to the project. Select node allocator. Use GPU Jupyter componet. Effectively unload Jupyter componet or reload a new one. Tensoboard #","title":"Project Page Tour"},{"location":"tutorials/project-page/#summary","text":"The last string of the project path identifies the project. Switch view advanced / simply Select experement. Project menu. Disable - unload all project components. Reset - delete project metadata (start from scratch) Edit - configure project including UI components. Delete - complelely delete project and all it`s metadata. Download - download project configuration. All project componentsare loaded and there is two of them. Create descriptionof then project, README, notes. Edit Summary.","title":"Summary"},{"location":"tutorials/project-page/#taks","text":"List of Task to execute as stand-alone or part of workflow. Edit task list. Select SAVE or SAVE AND EXECUTE to save this task configuration or create and start a new Job Load task template. Volume revision. Configuration of the Task covers storage, environment variables, resources and command line execution environment for your scripts.","title":"Taks"},{"location":"tutorials/project-page/#sources","text":"Tab gives access to the storage. NFS, S3, GIT, KL Cluster Storage, Model and Dataset from KL Catalog exposed as a storage volume available to All components of then project. Add and configure volumes","title":"Sources"},{"location":"tutorials/project-page/#history","text":"List of experiments and check-ins seved on the Tasks tab. Open dialog check-out version. By selectinga record in history list you can check-out the configuration for that record","title":"History"},{"location":"tutorials/project-page/#jobs","text":"Item from the list of executed Jobs. Job status. Log from worker task.","title":"Jobs"},{"location":"tutorials/project-page/#metrics","text":"Selector to monitoring tasks, ui components or serving. Selector metrics. Telemetry Monitoring for specific Job and component executed.","title":"Metrics"},{"location":"tutorials/project-page/#lib","text":"Manager package selector. User can install additional packages, not pre-configured into evironment template. Multiple packages can be specified as a comma separated list. Chackbox allows to see the complete list of packages installed in the environment.","title":"Lib"},{"location":"tutorials/project-page/#status","text":"Tab displays the status of components of the project. Jupyter, Tensorbord, Serving and Worker components for batch operations. If for some reason KL was not able to load specific component, the diagnostic messages will be provided.","title":"Status"},{"location":"tutorials/project-page/#jupyter","text":"Jupyter Notebook UI interface to the project. Select node allocator. Use GPU Jupyter componet. Effectively unload Jupyter componet or reload a new one.","title":"Jupyter"},{"location":"tutorials/project-page/#tensoboard","text":"","title":"Tensoboard"}]}